<!DOCTYPE html>
<html lang="en"><head>
<script src="04_convnets_0_files/libs/clipboard/clipboard.min.js"></script>
<script src="04_convnets_0_files/libs/quarto-html/tabby.min.js"></script>
<script src="04_convnets_0_files/libs/quarto-html/popper.min.js"></script>
<script src="04_convnets_0_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="04_convnets_0_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="04_convnets_0_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="04_convnets_0_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="04_convnets_0_files/libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.450">

  <title>convnets_0</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="04_convnets_0_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="04_convnets_0_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
        color: #97947a;
      }
    pre.numberSource { margin-left: 3em; border-left: 1px solid #97947a;  padding-left: 4px; }
    div.sourceCode
      { color: #f8f8f2; background-color: #2b2b2b; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #f8f8f2; } /* Normal */
    code span.al { color: #dcc6e0; } /* Alert */
    code span.an { color: #d4d0ab; } /* Annotation */
    code span.at { color: #ffd700; } /* Attribute */
    code span.bn { color: #dcc6e0; } /* BaseN */
    code span.bu { color: #f5ab35; } /* BuiltIn */
    code span.cf { color: #ffa07a; } /* ControlFlow */
    code span.ch { color: #abe338; } /* Char */
    code span.cn { color: #ffa07a; } /* Constant */
    code span.co { color: #d4d0ab; } /* Comment */
    code span.cv { color: #d4d0ab; font-style: italic; } /* CommentVar */
    code span.do { color: #d4d0ab; font-style: italic; } /* Documentation */
    code span.dt { color: #dcc6e0; } /* DataType */
    code span.dv { color: #dcc6e0; } /* DecVal */
    code span.er { color: #dcc6e0; } /* Error */
    code span.ex { color: #ffd700; } /* Extension */
    code span.fl { color: #f5ab35; } /* Float */
    code span.fu { color: #ffd700; } /* Function */
    code span.im { color: #f8f8f2; } /* Import */
    code span.in { color: #d4d0ab; } /* Information */
    code span.kw { color: #ffa07a; } /* Keyword */
    code span.op { color: #00e0e0; } /* Operator */
    code span.ot { color: #ffa07a; } /* Other */
    code span.pp { color: #dcc6e0; } /* Preprocessor */
    code span.sc { color: #00e0e0; } /* SpecialChar */
    code span.ss { color: #abe338; } /* SpecialString */
    code span.st { color: #abe338; } /* String */
    code span.va { color: #f5ab35; } /* Variable */
    code span.vs { color: #abe338; } /* VerbatimString */
    code span.wa { color: #d4d0ab; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="04_convnets_0_files/libs/revealjs/dist/theme/quarto.css">
  <link href="04_convnets_0_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="04_convnets_0_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="04_convnets_0_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="04_convnets_0_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <meta name="twitter:title" content="Outrageously efficient exploratory data analysis with Apache Arrow and dplyr">
  <meta name="twitter:description" content="A 10 minute lightning talk on all things arrow + dplyr">
  <meta name="twitter:url" content="https://jthomasmock.github.io/arrow-dplyr/#/">
  <meta name="twitter:image" content="https://raw.githubusercontent.com/jthomasmock/arrow-dplyr/master/index-img.png">
  <meta name="twitter:image:alt" content="The title slide of the presentation, with the arrow and dplyr hex logos">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:creator" content="@thomas_mock">
  <meta name="twitter:site" content="@thomas_mock">
  <meta property="og:title" content="Outrageously efficient exploratory data analysis with Apache Arrow and dplyr">
  <meta property="og:description" content="A 10 minute lightning talk on all things arrow + dplyr">
  <meta property="og:url" content="https://jthomasmock.github.io/arrow-dplyr/#/">
  <meta property="og:image" content="https://raw.githubusercontent.com/jthomasmock/arrow-dplyr/master/index-img.png">
  <meta property="og:image:alt" content="The title slide of the presentation, with the arrow and dplyr hex logos">
  <meta property="og:type" content="website">
  <meta property="og:locale" content="en_US">
</head>
<body class="quarto-dark">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block">
  <h1 class="title">Convolutional Neural Networks 0</h1>

<div class="quarto-title-authors">
</div>

</section>
<section class="slide level2">

<br>
<h2>
Data Science in Electron Microscopy
</h2>
<hr>
<h3>
Philipp Pelz
</h3>
<h3>
2024
</h3>
<p><br></p>
<h3>
&nbsp; <a href="https://github.com/ECLIPSE-Lab/SS24_DataScienceForEM">https://github.com/ECLIPSE-Lab/SS24_DataScienceForEM</a>
</h3>
</section>
<section id="from-fully-connected-layers-to-convolutions" class="slide level2">
<h2>From Fully Connected Layers to Convolutions</h2>
<p>:label:<code>sec_why-conv</code></p>
<ul>
<li>models that we have discussed so far remain appropriate options when we are dealing with tabular data.</li>
<li>tabular: data consist of rows corresponding to examples and columns corresponding to features.</li>
<li>With tabular data, we might anticipate that the patterns we seek could involve interactions among the features, but we do not assume any structure <em>a priori</em> concerning how the features interact.</li>
<li>no prior knowledge about structure of data –&gt; MLP</li>
<li>for high-dimensional perceptual data, structure-less networks can grow unwieldy.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>let’s return to our running exampleof distinguishing cats from dogs.</li>
<li>Say that we do a thorough job in data collection, collecting an annotated dataset of one-megapixel photographs.</li>
<li><span class="math inline">\(\rightarrow\)</span> each input to the network has one million dimensions.</li>
<li>aggressive reduction to one thousand hidden dimensions <span class="math inline">\(\rightarrow\)</span> fully connected layer with <span class="math inline">\(10^6 \times 10^3 = 10^9\)</span> parameters.</li>
<li>one megapixel resolution may not be necessary</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>might be able to get away with one hundred thousand pixels, our hidden layer of size 1000 grossly underestimates the number of hidden units that it takes to learn good representations of images <span class="math inline">\(\rightarrow\)</span> practical system will still require billions of parameters.</li>
<li>learning a classifier by fitting so many parameters might require collecting an enormous dataset.</li>
<li>images exhibit rich structure that can be exploited.</li>
<li>Convolutional neural networks (CNNs) are one way for exploiting some of the known structure in natural images.</li>
</ul>
</section>
<section id="invariance" class="slide level2">
<h2>Invariance</h2>
<ul>
<li>want to detect an object in an image.</li>
<li>seems reasonable that whatever method we use to recognize objects should not be overly concerned with the precise location of the object in the image.</li>
<li>system should exploit this knowledge. Pigs usually do not fly and planes usually do not swim. Nonetheless, we should still recognize a pig were one to appear at the top of the image.</li>
<li>We can draw some inspiration here from the children’s game “Where’s Waldo” (depicted in :numref:<code>img_waldo</code>).</li>
<li>The game consists of a number of chaotic scenes bursting with activities.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>Waldo shows up somewhere in each, typically lurking in some unlikely location. The reader’s goal is to locate him. Despite his characteristic outfit, this can be surprisingly difficult, due to the large number of distractions.</li>
<li>However, <em>what Waldo looks like</em> does not depend upon <em>where Waldo is located</em>. We could sweep the image with a Waldo detector that could assign a score to each patch, indicating the likelihood that the patch contains Waldo.</li>
<li>In fact, many object detection and segmentation algorithms are based on this approach :cite:<code>Long.Shelhamer.Darrell.2015</code>.</li>
<li>CNNs systematize this idea of <em>spatial invariance</em>, exploiting it to learn useful representations with fewer parameters.</li>
</ul>
</section>
<section class="slide level2">


<img data-src="../img/where-wally-walker-books.jpg" class="r-stretch quarto-figure-center"><p class="caption">An image of the “Where’s Waldo” game.</p><ul>
<li>We can now make these intuitions more concrete by enumerating a few desiderata to guide our design of a neural network architecture suitable for computer vision:</li>
</ul>
</section>
<section class="slide level2">

<ol type="1">
<li>In the earliest layers, our network should respond similarly to the same patch, regardless of where it appears in the image. This principle is called <em>translation invariance</em> (or <em>translation equivariance</em>).</li>
<li>The earliest layers of the network should focus on local regions, without regard for the contents of the image in distant regions. This is the <em>locality</em> principle. Eventually, these local representations can be aggregated to make predictions at the whole image level.</li>
<li>As we proceed, deeper layers should be able to capture longer-range features of the image, in a way similar to higher level vision in nature.</li>
</ol>
<ul>
<li>Let’s see how this translates into mathematics.</li>
</ul>
</section>
<section id="constraining-the-mlp" class="slide level2">
<h2>Constraining the MLP</h2>
<ul>
<li><p>To start off, we can consider an MLP with two-dimensional images <span class="math inline">\(\mathbf{X}\)</span> as inputs and their immediate hidden representations <span class="math inline">\(\mathbf{H}\)</span> similarly represented as matrices (they are two-dimensional tensors in code), where both <span class="math inline">\(\mathbf{X}\)</span> and <span class="math inline">\(\mathbf{H}\)</span> have the same shape.</p></li>
<li><p>Let that sink in. We now conceive of not only the inputs but also the hidden representations as possessing spatial structure.</p></li>
<li><p>Let <span class="math inline">\([\mathbf{X}]_{i, j}\)</span> and <span class="math inline">\([\mathbf{H}]_{i, j}\)</span> denote the pixel at location <span class="math inline">\((i,j)\)</span> in the input image and hidden representation, respectively.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>Consequently, to have each of the hidden units receive input from each of the input pixels, we would switch from using weight matrices (as we did previously in MLPs) to representing our parameters as fourth-order weight tensors <span class="math inline">\(\mathsf{W}\)</span>.</li>
<li>Suppose that <span class="math inline">\(\mathbf{U}\)</span> contains biases, we could formally express the fully connected layer as</li>
</ul>
<p><span class="math display">\[\begin{aligned} \left[\mathbf{H}\right]_{i, j} &amp;= [\mathbf{U}]_{i, j} + \sum_k \sum_l[\mathsf{W}]_{i, j, k, l}  [\mathbf{X}]_{k, l}\\ &amp;=  [\mathbf{U}]_{i, j} +
\sum_a \sum_b [\mathsf{V}]_{i, j, a, b}  [\mathbf{X}]_{i+a, j+b}.\end{aligned}\]</span></p>
<ul>
<li>switch from <span class="math inline">\(\mathsf{W}\)</span> to <span class="math inline">\(\mathsf{V}\)</span> is entirely cosmetic for now since there is a one-to-one correspondence between coefficients in both fourth-order tensors.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>We simply re-index the subscripts <span class="math inline">\((k, l)\)</span> such that <span class="math inline">\(k = i+a\)</span> and <span class="math inline">\(l = j+b\)</span>. In other words, we set <span class="math inline">\([\mathsf{V}]_{i, j, a, b} = [\mathsf{W}]_{i, j, i+a, j+b}\)</span>.</li>
<li>The indices <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> run over both positive and negative offsets, covering the entire image. For any given location (<span class="math inline">\(i\)</span>, <span class="math inline">\(j\)</span>) in the hidden representation <span class="math inline">\([\mathbf{H}]_{i, j}\)</span>, we compute its value by summing over pixels in <span class="math inline">\(x\)</span>, centered around <span class="math inline">\((i, j)\)</span> and weighted by <span class="math inline">\([\mathsf{V}]_{i, j, a, b}\)</span>.</li>
<li>Before we carry on, let’s consider the total number of parameters required for a <em>single</em> layer in this parametrization: a <span class="math inline">\(1000 \times 1000\)</span> image (1 megapixel) is mapped to a <span class="math inline">\(1000 \times 1000\)</span> hidden representation. This requires <span class="math inline">\(10^{12}\)</span> parameters, far beyond what computers currently can handle.</li>
</ul>
</section>
<section id="translation-invariance" class="slide level2">
<h2>Translation Invariance</h2>
<ul>
<li>invoke the first principle established above: translation invariance :cite:<code>Zhang.ea.1988</code>.</li>
<li>This implies that a shift in the input <span class="math inline">\(\mathbf{X}\)</span> should simply lead to a shift in the hidden representation <span class="math inline">\(\mathbf{H}\)</span>.</li>
<li>This is only possible if <span class="math inline">\(\mathsf{V}\)</span> and <span class="math inline">\(\mathbf{U}\)</span> do not actually depend on <span class="math inline">\((i, j)\)</span>. As such, we have <span class="math inline">\([\mathsf{V}]_{i, j, a, b} = [\mathbf{V}]_{a, b}\)</span> and <span class="math inline">\(\mathbf{U}\)</span> is a constant, say <span class="math inline">\(u\)</span>.</li>
<li>As a result, we can simplify the definition for <span class="math inline">\(\mathbf{H}\)</span>:</li>
</ul>
<p><span class="math display">\[[\mathbf{H}]_{i, j} = u + \sum_a\sum_b [\mathbf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}.\]</span></p>
</section>
<section class="slide level2">

<ul>
<li>This is a <em>convolution</em>! We are effectively weighting pixels at <span class="math inline">\((i+a, j+b)\)</span> in the vicinity of location <span class="math inline">\((i, j)\)</span> with coefficients <span class="math inline">\([\mathbf{V}]_{a, b}\)</span> to obtain the value <span class="math inline">\([\mathbf{H}]_{i, j}\)</span>.</li>
<li>Note that <span class="math inline">\([\mathbf{V}]_{a, b}\)</span> needs many fewer coefficients than <span class="math inline">\([\mathsf{V}]_{i, j, a, b}\)</span> since it no longer depends on the location within the image. Consequently, the number of parameters required is no longer <span class="math inline">\(10^{12}\)</span> but a much more reasonable <span class="math inline">\(4 \cdot 10^6\)</span>: we still have the dependency on <span class="math inline">\(a, b \in (-1000, 1000)\)</span>.</li>
<li>In short, we have made significant progress. Time-delay neural networks (TDNNs) are some of the first examples to exploit this idea :cite:<code>Waibel.Hanazawa.Hinton.ea.1989</code>.</li>
</ul>
</section>
<section id="locality" class="slide level2">
<h2>Locality</h2>
<ul>
<li>Now let’s invoke the second principle: locality. As motivated above, we believe that we should not have to look very far away from location <span class="math inline">\((i, j)\)</span> in order to glean relevant information to assess what is going on at <span class="math inline">\([\mathbf{H}]_{i, j}\)</span>.</li>
<li>This means that outside some range <span class="math inline">\(|a|&gt; \Delta\)</span> or <span class="math inline">\(|b| &gt; \Delta\)</span>, we should set <span class="math inline">\([\mathbf{V}]_{a, b} = 0\)</span>.</li>
<li>Equivalently, we can rewrite <span class="math inline">\([\mathbf{H}]_{i, j}\)</span> as <span class="math display">\[[\mathbf{H}]_{i, j} = u + \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} [\mathbf{V}]_{a, b}  [\mathbf{X}]_{i+a, j+b}.\]</span> :eqlabel:<code>eq_conv-layer</code></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>This reduces the number of parameters from <span class="math inline">\(4 \cdot 10^6\)</span> to <span class="math inline">\(4 \Delta^2\)</span>, where <span class="math inline">\(\Delta\)</span> is typically smaller than <span class="math inline">\(10\)</span>. As such, we reduced the number of parameters by another 4 orders of magnitude. Note that :eqref:<code>eq_conv-layer</code>, in a nutshell, is what is called a <em>convolutional layer</em>.</li>
<li><em>Convolutional neural networks</em> (CNNs) are a special family of neural networks that contain convolutional layers. In the deep learning research community, <span class="math inline">\(\mathbf{V}\)</span> is referred to as a <em>convolution kernel</em>, a <em>filter</em>, or simply the layer’s <em>weights</em> that are learnable parameters.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>While previously, we might have required billions of parameters to represent just a single layer in an image-processing network, we now typically need just a few hundred, without altering the dimensionality of either the inputs or the hidden representations.</li>
<li>The price paid for this drastic reduction in parameters is that our features are now translation invariant and that our layer can only incorporate local information, when determining the value of each hidden activation.</li>
<li>All learning depends on imposing inductive bias. When that bias agrees with reality, we get sample-efficient models that generalize well to unseen data.</li>
<li>But of course, if those biases do not agree with reality, e.g., if images turned out not to be translation invariant, our models might struggle even to fit our training data.</li>
<li>This dramatic reduction in parameters brings us to our last desideratum, namely that deeper layers should represent larger and more complex aspects of an image. This can be achieved by interleaving nonlinearities and convolutional layers repeatedly.</li>
</ul>
</section>
<section id="convolutions" class="slide level2">
<h2>Convolutions</h2>
<ul>
<li>Let’s briefly review why :eqref:<code>eq_conv-layer</code> is called a convolution.</li>
<li>In mathematics, the <em>convolution</em> between two functions :cite:<code>Rudin.1973</code>, say <span class="math inline">\(f, g: \mathbb{R}^d \to \mathbb{R}\)</span> is defined as</li>
</ul>
<p><span class="math display">\[(f * g)(\mathbf{x}) = \int f(\mathbf{z}) g(\mathbf{x}-\mathbf{z}) d\mathbf{z}.\]</span></p>
<ul>
<li>That is, we measure the overlap between <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> when one function is “flipped” and shifted by <span class="math inline">\(\mathbf{x}\)</span>.</li>
<li>Whenever we have discrete objects, the integral turns into a sum. For instance, for vectors from the set of square summable infinite dimensional vectors with index running over <span class="math inline">\(\mathbb{Z}\)</span> we obtain the following definition:</li>
</ul>
</section>
<section class="slide level2">

<p><span class="math display">\[(f * g)(i) = \sum_a f(a) g(i-a).\]</span></p>
<ul>
<li>For two-dimensional tensors, we have a corresponding sum with indices <span class="math inline">\((a, b)\)</span> for <span class="math inline">\(f\)</span> and <span class="math inline">\((i-a, j-b)\)</span> for <span class="math inline">\(g\)</span>, respectively:</li>
<li><span class="math display">\[(f * g)(i, j) = \sum_a\sum_b f(a, b) g(i-a, j-b).\]</span> :eqlabel:<code>eq_2d-conv-discrete</code></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>This looks similar to :eqref:<code>eq_conv-layer</code>, with one major difference.</li>
<li>Rather than using <span class="math inline">\((i+a, j+b)\)</span>, we are using the difference instead. Note, though, that this distinction is mostly cosmetic since we can always match the notation between :eqref:<code>eq_conv-layer</code> and :eqref:<code>eq_2d-conv-discrete</code>.</li>
<li>Our original definition in :eqref:<code>eq_conv-layer</code> more properly describes a <em>cross-correlation</em>. We will come back to this in the following section.</li>
</ul>
</section>
<section id="channels" class="slide level2">
<h2>Channels</h2>
<ul>
<li>Returning to Waldo detector, let’s see what this looks like.</li>
<li>convolutional layer picks windows of a given size and weighs intensities according to the filter <span class="math inline">\(\mathsf{V}\)</span>, as demonstrated in :numref:<code>fig_waldo_mask</code>.</li>
<li>aim to learn a model so that wherever the “waldoness” is highest, we should find a peak in the hidden layer representations.</li>
</ul>

<img data-src="../img/waldo-mask.jpg" width="400" class="r-stretch quarto-figure-center"><p class="caption">Detect Waldo.</p></section>
<section class="slide level2">

<ul>
<li>There is just one problem with this approach. So far, we blissfully ignored that images consist of 3 channels: red, green, and blue.<br>
</li>
<li>In sum, images are not two-dimensional objects but rather third-order tensors, characterized by a height, width, and channel, e.g., with shape <span class="math inline">\(1024 \times 1024 \times 3\)</span> pixels.<br>
</li>
<li>While the first two of these axes concern spatial relationships, the third can be regarded as assigning a multidimensional representation to each pixel location.</li>
<li>We thus index <span class="math inline">\(\mathsf{X}\)</span> as <span class="math inline">\([\mathsf{X}]_{i, j, k}\)</span>. The convolutional filter has to adapt accordingly. Instead of <span class="math inline">\([\mathbf{V}]_{a,b}\)</span>, we now have <span class="math inline">\([\mathsf{V}]_{a,b,c}\)</span>.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>Moreover, just as our input consists of a third-order tensor, it turns out to be a good idea to similarly formulate our hidden representations as third-order tensors <span class="math inline">\(\mathsf{H}\)</span>.</li>
<li>In other words, rather than just having a single hidden representation corresponding to each spatial location, we want an entire vector of hidden representations corresponding to each spatial location.</li>
<li>We could think of the hidden representations as comprising a number of two-dimensional grids stacked on top of each other. As in the inputs, these are sometimes called <em>channels</em>.</li>
<li>They are also sometimes called <em>feature maps</em>, as each provides a spatialized set of learned features to the subsequent layer. Intuitively, you might imagine that at lower layers that are closer to inputs, some channels could become specialized to recognize edges while others could recognize textures.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>To support multiple channels in both inputs (<span class="math inline">\(\mathsf{X}\)</span>) and hidden representations (<span class="math inline">\(\mathsf{H}\)</span>), we can add a fourth coordinate to <span class="math inline">\(\mathsf{V}\)</span>: <span class="math inline">\([\mathsf{V}]_{a, b, c, d}\)</span>.</li>
<li>Putting everything together we have:</li>
</ul>
<p><span class="math display">\[[\mathsf{H}]_{i,j,d} = \sum_{a = -\Delta}^{\Delta} \sum_{b = -\Delta}^{\Delta} \sum_c [\mathsf{V}]_{a, b, c, d} [\mathsf{X}]_{i+a, j+b, c},\]</span> :eqlabel:<code>eq_conv-layer-channels</code></p>
<ul>
<li>where <span class="math inline">\(d\)</span> indexes the output channels in the hidden representations <span class="math inline">\(\mathsf{H}\)</span>. The subsequent convolutional layer will go on to take a third-order tensor, <span class="math inline">\(\mathsf{H}\)</span>, as input.</li>
<li>Being more general, :eqref:<code>eq_conv-layer-channels</code> is the definition of a convolutional layer for multiple channels, where <span class="math inline">\(\mathsf{V}\)</span> is a kernel or filter of the layer.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>still many operations that we need to address.</li>
<li>For instance, we need to figure out how to combine all the hidden representations to a single output, e.g., whether there is a Waldo <em>anywhere</em> in the image.</li>
<li>We also need to decide how to compute things efficiently, how to combine multiple layers, appropriate activation functions, and how to make reasonable design choices to yield networks that are effective in practice. We turn to these issues in the remainder of the chapter.</li>
</ul>
</section>
<section id="summary-and-discussion" class="slide level2">
<h2>Summary and Discussion</h2>
<ul>
<li>derived the structure of convolutional neural networks from first principles.</li>
<li>-unclear whether this is what led to the invention of CNNs, it is satisfying to know that they are the <em>right</em> choice when applying reasonable principles to how image processing and computer vision algorithms should operate, at least at lower levels.</li>
<li>In particular, translation invariance in images implies that all patches of an image will be treated in the same manner.</li>
<li>Locality means that only a small neighborhood of pixels will be used to compute the corresponding hidden representations. Some of the earliest references to CNNs are in the form of the Neocognitron :cite:<code>Fukushima.1982</code>.</li>
<li>second principle that we encountered in our reasoning is how to reduce the number of parameters in a function class without limiting its expressive power, at least, whenever certain assumptions on the model hold.</li>
<li>saw a dramatic reduction of complexity as a result of this restriction, turning computationally and statistically infeasible problems into tractable models.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>Adding channels allowed us to bring back some of the complexity that was lost due to the restrictions imposed on the convolutional kernel by locality and translation invariance. Note that channels are quite a natural addition beyond red, green, and blue.</li>
<li>Many images have tens to hundreds of channels, generating hyperspectral images instead. They report data on many different wavelengths. In the following we will see how to use convolutions effectively to manipulate the dimensionality of the images they operate on, how to move from location-based to channel-based representations and how to deal with large numbers of categories efficiently.</li>
</ul>
</section>
<section id="exercises" class="slide level2">
<h2>Exercises</h2>
<ol type="1">
<li>Assume that the size of the convolution kernel is <span class="math inline">\(\Delta = 0\)</span>. Show that in this case the convolution kernel implements an MLP independently for each set of channels. This leads to the Network in Network architectures :cite:<code>Lin.Chen.Yan.2013</code>.</li>
<li>Audio data is often represented as a one-dimensional sequence.
<ol type="1">
<li>When might you want to impose locality and translation invariance for audio?</li>
<li>Derive the convolution operations for audio.</li>
<li>Can you treat audio using the same tools as computer vision? Hint: use the spectrogram.</li>
</ol></li>
<li>Why might translation invariance not be a good idea after all? Give an example.</li>
<li>Do you think that convolutional layers might also be applicable for text data? Which problems might you encounter with language?</li>
<li>What happens with convolutions when an object is at the boundary of an image.</li>
<li>Prove that the convolution is symmetric, i.e., <span class="math inline">\(f * g = g * f\)</span>.</li>
<li>Prove the convolution theorem, i.e., <span class="math inline">\(f * g = \mathcal{F}^{-1}\left[\mathcal{F}[f] \cdot \mathcal{F}[g]\right]\)</span>. Can you use it to accelerate convolutions?</li>
</ol>
</section>
<section id="convolutions-for-images" class="slide level2">
<h2>Convolutions for Images</h2>
<ul>
<li>understand how convolutional layers work in theory, we are ready to see how they work in practice.</li>
<li>Building on our motivation of convolutional neural networks as efficient architectures for exploring structure in image data, we stick with images as our running example.</li>
</ul>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">from</span> d2l <span class="im">import</span> torch <span class="im">as</span> d2l</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">from</span> torch <span class="im">import</span> nn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="the-cross-correlation-operation" class="slide level2">
<h2>The Cross-Correlation Operation</h2>
<ul>
<li><p>Recall that strictly speaking, convolutional layers are a misnomer, since the operations they express are more accurately described as cross-correlations.</p></li>
<li><p>Based on our descriptions of convolutional layers in :numref:<code>sec_why-conv</code>, in such a layer, an input tensor and a kernel tensor are combined to produce an output tensor through a (<strong>cross-correlation operation.</strong>)</p></li>
<li><p>ignore channels for now and see how this works with two-dimensional data and hidden representations.</p></li>
<li><p>In :numref:<code>fig_correlation</code>, the input is a two-dimensional tensor with a height of 3 and width of 3. We mark the shape of the tensor as <span class="math inline">\(3 \times 3\)</span> or (<span class="math inline">\(3\)</span>, <span class="math inline">\(3\)</span>). The height and width of the kernel are both 2. The shape of the <em>kernel window</em> (or <em>convolution window</em>) is given by the height and width of the kernel (here it is <span class="math inline">\(2 \times 2\)</span>).</p></li>
</ul>
</section>
<section class="slide level2">


<img data-src="../img/correlation.svg" class="r-stretch quarto-figure-center"><p class="caption">Two-dimensional cross-correlation operation. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: <span class="math inline">\(0\times0+1\times1+3\times2+4\times3=19\)</span>.</p><ul>
<li>In the two-dimensional cross-correlation operation, we begin with the convolution window positioned at the upper-left corner of the input tensor and slide it across the input tensor, both from left to right and top to bottom.</li>
<li>When the convolution window slides to a certain position, the input subtensor contained in that window and the kernel tensor are multiplied elementwise and the resulting tensor is summed up yielding a single scalar value.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>This result gives the value of the output tensor at the corresponding location. Here, the output tensor has a height of 2 and width of 2 and the four elements are derived from the two-dimensional cross-correlation operation:</li>
</ul>
<p><span class="math display">\[
0\times0+1\times1+3\times2+4\times3=19,\\
1\times0+2\times1+4\times2+5\times3=25,\\
3\times0+4\times1+6\times2+7\times3=37,\\
4\times0+5\times1+7\times2+8\times3=43.
\]</span></p>
<ul>
<li>along each axis, the output size is slightly smaller than the input size.</li>
<li>Because the kernel has width and height greater than one, we can only properly compute the cross-correlation for locations where the kernel fits wholly within the image, the output size is given by the input size <span class="math inline">\(n_h \times n_w\)</span> minus the size of the convolution kernel <span class="math inline">\(k_h \times k_w\)</span> via</li>
</ul>
<p><span class="math display">\[(n_h-k_h+1) \times (n_w-k_w+1).\]</span></p>
</section>
<section class="slide level2">

<ul>
<li>This is the case since we need enough space to “shift” the convolution kernel across the image. Later we will see how to keep the size unchanged by padding the image with zeros around its boundary so that there is enough space to shift the kernel. Next, we implement this process in the <code>corr2d</code> function, which accepts an input tensor <code>X</code> and a kernel tensor <code>K</code> and returns an output tensor <code>Y</code>.</li>
</ul>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1"></a><span class="kw">def</span> corr2d(X, K):  <span class="co">#@save</span></span>
<span id="cb2-2"><a href="#cb2-2"></a>    <span class="co">"""Compute 2D cross-correlation."""</span></span>
<span id="cb2-3"><a href="#cb2-3"></a>    h, w <span class="op">=</span> K.shape</span>
<span id="cb2-4"><a href="#cb2-4"></a>    Y <span class="op">=</span> d2l.zeros((X.shape[<span class="dv">0</span>] <span class="op">-</span> h <span class="op">+</span> <span class="dv">1</span>, X.shape[<span class="dv">1</span>] <span class="op">-</span> w <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb2-5"><a href="#cb2-5"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(Y.shape[<span class="dv">0</span>]):</span>
<span id="cb2-6"><a href="#cb2-6"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(Y.shape[<span class="dv">1</span>]):</span>
<span id="cb2-7"><a href="#cb2-7"></a>            Y[i, j] <span class="op">=</span> d2l.reduce_sum((X[i: i <span class="op">+</span> h, j: j <span class="op">+</span> w] <span class="op">*</span> K))</span>
<span id="cb2-8"><a href="#cb2-8"></a>    <span class="cf">return</span> Y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section class="slide level2">

<ul>
<li>We can construct the input tensor <code>X</code> and the kernel tensor <code>K</code> from :numref:<code>fig_correlation</code> to [<strong>validate the output of the above implementation</strong>] of the two-dimensional cross-correlation operation.</li>
</ul>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a>X <span class="op">=</span> d2l.tensor([[<span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">2.0</span>], [<span class="fl">3.0</span>, <span class="fl">4.0</span>, <span class="fl">5.0</span>], [<span class="fl">6.0</span>, <span class="fl">7.0</span>, <span class="fl">8.0</span>]])</span>
<span id="cb3-2"><a href="#cb3-2"></a>K <span class="op">=</span> d2l.tensor([[<span class="fl">0.0</span>, <span class="fl">1.0</span>], [<span class="fl">2.0</span>, <span class="fl">3.0</span>]])</span>
<span id="cb3-3"><a href="#cb3-3"></a>corr2d(X, K)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>tensor([[19., 25.],
        [37., 43.]])</code></pre>
</div>
</div>
</section>
<section id="convolutional-layers" class="slide level2">
<h2>Convolutional Layers</h2>
<ul>
<li><p>convolutional layer cross-correlates the input and kernel and adds a scalar bias to produce an output.</p></li>
<li><p>The two parameters of a convolutional layer are the kernel and the scalar bias.</p></li>
<li><p>When training models based on convolutional layers, we typically initialize the kernels randomly, just as we would with a fully connected layer.</p></li>
<li><p>We are now ready to [<strong>implement a two-dimensional convolutional layer</strong>] based on the <code>corr2d</code> function defined above.</p></li>
<li><p>In the <code>__init__</code> constructor method, we declare <code>weight</code> and <code>bias</code> as the two model parameters. The forward propagation method calls the <code>corr2d</code> function and adds the bias.</p></li>
</ul>
</section>
<section class="slide level2">

<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a><span class="kw">class</span> Conv2D(nn.Module):</span>
<span id="cb5-2"><a href="#cb5-2"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, kernel_size):</span>
<span id="cb5-3"><a href="#cb5-3"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-4"><a href="#cb5-4"></a>        <span class="va">self</span>.weight <span class="op">=</span> nn.Parameter(torch.rand(kernel_size))</span>
<span id="cb5-5"><a href="#cb5-5"></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros(<span class="dv">1</span>))</span>
<span id="cb5-6"><a href="#cb5-6"></a></span>
<span id="cb5-7"><a href="#cb5-7"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-8"><a href="#cb5-8"></a>        <span class="cf">return</span> corr2d(x, <span class="va">self</span>.weight) <span class="op">+</span> <span class="va">self</span>.bias</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>In <span class="math inline">\(h \times w\)</span> convolution or a <span class="math inline">\(h \times w\)</span> convolution kernel, the height and width of the convolution kernel are <span class="math inline">\(h\)</span> and <span class="math inline">\(w\)</span>, respectively.</li>
<li>We also refer to a convolutional layer with a <span class="math inline">\(h \times w\)</span> convolution kernel simply as a <span class="math inline">\(h \times w\)</span> convolutional layer.</li>
</ul>
</section>
<section id="object-edge-detection-in-images" class="slide level2">
<h2>Object Edge Detection in Images</h2>
<ul>
<li>take a moment to parse [<strong>a simple application of a convolutional layer: detecting the edge of an object in an image</strong>] by finding the location of the pixel change.</li>
<li>First, we construct an “image” of <span class="math inline">\(6\times 8\)</span> pixels. The middle four columns are black (0) and the rest are white (1).</li>
</ul>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>X <span class="op">=</span> d2l.ones((<span class="dv">6</span>, <span class="dv">8</span>))</span>
<span id="cb6-2"><a href="#cb6-2"></a>X[:, <span class="dv">2</span>:<span class="dv">6</span>] <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb6-3"><a href="#cb6-3"></a>X</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>tensor([[1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.],
        [1., 1., 0., 0., 0., 0., 1., 1.]])</code></pre>
</div>
</div>
<ul>
<li>next construct a kernel <code>K</code> with a height of 1 and a width of 2.</li>
<li>perform the cross-correlation operation with the input, if the horizontally adjacent elements are the same, the output is 0. Otherwise, the output is non-zero.</li>
<li>kernel is special case of a finite difference operator. At location <span class="math inline">\((i,j)\)</span> it computes <span class="math inline">\(x_{i,j} - x_{(i+1),j}\)</span>, i.e., it computes the difference between the values of horizontally adjacent pixels.</li>
<li>discrete approximation of the first derivative in the horizontal direction. After all, for a function <span class="math inline">\(f(i,j)\)</span> its derivative <span class="math inline">\(-\partial_i f(i,j) = \lim_{\epsilon \to 0} \frac{f(i,j) - f(i+\epsilon,j)}{\epsilon}\)</span>. Let’s see how this works in practice.</li>
</ul>
</section>
<section class="slide level2">

<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1"></a>K <span class="op">=</span> d2l.tensor([[<span class="fl">1.0</span>, <span class="op">-</span><span class="fl">1.0</span>]])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>ready to perform the cross-correlation operation with arguments <code>X</code> (our input) and <code>K</code> (our kernel). As you can see, [<strong>we detect 1 for the edge from white to black and -1 for the edge from black to white.</strong>] All other outputs take value 0.</li>
</ul>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1"></a>Y <span class="op">=</span> corr2d(X, K)</span>
<span id="cb9-2"><a href="#cb9-2"></a>Y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],
        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])</code></pre>
</div>
</div>
<ul>
<li>We can now apply the kernel to the transposed image. As expected, it vanishes. [<strong>The kernel <code>K</code> only detects vertical edges.</strong>]</li>
</ul>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>corr2d(d2l.transpose(X), K)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>tensor([[0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.],
        [0., 0., 0., 0., 0.]])</code></pre>
</div>
</div>
</section>
<section id="learning-a-kernel" class="slide level2">
<h2>Learning a Kernel</h2>
<ul>
<li>Designing an edge detector by finite differences <code>[1, -1]</code> is neat if we know this is precisely what we are looking for. However, as we look at larger kernels, and consider successive layers of convolutions, it might be impossible to specify precisely what each filter should be doing manually.</li>
<li>Now let’s see whether we can [<strong>learn the kernel that generated <code>Y</code> from <code>X</code></strong>] by looking at the input–output pairs only.</li>
<li>We first construct a convolutional layer and initialize its kernel as a random tensor. Next, in each iteration, we will use the squared error to compare <code>Y</code> with the output of the convolutional layer.</li>
<li>We can then calculate the gradient to update the kernel. For the sake of simplicity, in the following we use the built-in class for two-dimensional convolutional layers and ignore the bias.</li>
</ul>
</section>
<section class="slide level2">

<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a><span class="co">## Construct a two-dimensional convolutional layer with 1 output channel and a</span></span>
<span id="cb13-2"><a href="#cb13-2"></a><span class="co">## kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here</span></span>
<span id="cb13-3"><a href="#cb13-3"></a>conv2d <span class="op">=</span> nn.LazyConv2d(<span class="dv">1</span>, kernel_size<span class="op">=</span>(<span class="dv">1</span>, <span class="dv">2</span>), bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb13-4"><a href="#cb13-4"></a></span>
<span id="cb13-5"><a href="#cb13-5"></a><span class="co">## The two-dimensional convolutional layer uses four-dimensional input and</span></span>
<span id="cb13-6"><a href="#cb13-6"></a><span class="co">## output in the format of (example, channel, height, width), where the batch</span></span>
<span id="cb13-7"><a href="#cb13-7"></a><span class="co">## size (number of examples in the batch) and the number of channels are both 1</span></span>
<span id="cb13-8"><a href="#cb13-8"></a>X <span class="op">=</span> X.reshape((<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">8</span>))</span>
<span id="cb13-9"><a href="#cb13-9"></a>Y <span class="op">=</span> Y.reshape((<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">6</span>, <span class="dv">7</span>))</span>
<span id="cb13-10"><a href="#cb13-10"></a>lr <span class="op">=</span> <span class="fl">3e-2</span>  <span class="co">## Learning rate</span></span>
<span id="cb13-11"><a href="#cb13-11"></a></span>
<span id="cb13-12"><a href="#cb13-12"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb13-13"><a href="#cb13-13"></a>    Y_hat <span class="op">=</span> conv2d(X)</span>
<span id="cb13-14"><a href="#cb13-14"></a>    l <span class="op">=</span> (Y_hat <span class="op">-</span> Y) <span class="op">**</span> <span class="dv">2</span></span>
<span id="cb13-15"><a href="#cb13-15"></a>    conv2d.zero_grad()</span>
<span id="cb13-16"><a href="#cb13-16"></a>    l.<span class="bu">sum</span>().backward()</span>
<span id="cb13-17"><a href="#cb13-17"></a>    <span class="co">## Update the kernel</span></span>
<span id="cb13-18"><a href="#cb13-18"></a>    conv2d.weight.data[:] <span class="op">-=</span> lr <span class="op">*</span> conv2d.weight.grad</span>
<span id="cb13-19"><a href="#cb13-19"></a>    <span class="cf">if</span> (i <span class="op">+</span> <span class="dv">1</span>) <span class="op">%</span> <span class="dv">2</span> <span class="op">==</span> <span class="dv">0</span>:</span>
<span id="cb13-20"><a href="#cb13-20"></a>        <span class="bu">print</span>(<span class="ss">f'epoch </span><span class="sc">{</span>i <span class="op">+</span> <span class="dv">1</span><span class="sc">}</span><span class="ss">, loss </span><span class="sc">{</span>l<span class="sc">.</span><span class="bu">sum</span>()<span class="sc">:.3f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>epoch 2, loss 1.890
epoch 4, loss 0.381
epoch 6, loss 0.090
epoch 8, loss 0.026
epoch 10, loss 0.009</code></pre>
</div>
</div>
<ul>
<li>Note that the error has dropped to a small value after 10 iterations. Now we will <strong>take a look at the kernel tensor we learned.</strong></li>
</ul>
</section>
<section class="slide level2">

<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1"></a>d2l.reshape(conv2d.weight.data, (<span class="dv">1</span>, <span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<pre><code>tensor([[ 0.9844, -1.0023]])</code></pre>
</div>
</div>
<ul>
<li>Indeed, the learned kernel tensor is remarkably close to the kernel tensor <code>K</code> we defined earlier.</li>
</ul>
</section>
<section id="cross-correlation-and-convolution" class="slide level2">
<h2>Cross-Correlation and Convolution</h2>
<ul>
<li>Recall our observation from :numref:<code>sec_why-conv</code> of the correspondence between the cross-correlation and convolution operations.</li>
<li>consider two-dimensional convolutional layers. What if such layers perform strict convolution operations as defined in :eqref:<code>eq_2d-conv-discrete</code> instead of cross-correlations?</li>
<li>for strict <em>convolution</em> operation, we only need to flip the two-dimensional kernel tensor both horizontally and vertically, and then perform the <em>cross-correlation</em> operation with the input tensor.</li>
<li>kernels are learned from data in deep learning –&gt; outputs of convolutional layers remain unaffected no matter such layers perform either the strict convolution operations or the cross-correlation operations.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>suppose that a convolutional layer performs <em>cross-correlation</em> and learns the kernel in :numref:<code>fig_correlation</code>, which is denoted as the matrix <span class="math inline">\(\mathbf{K}\)</span> here.</li>
<li>Assuming that other conditions remain unchanged, when this layer performs strict <em>convolution</em> instead, the learned kernel <span class="math inline">\(\mathbf{K}'\)</span> will be the same as <span class="math inline">\(\mathbf{K}\)</span> after <span class="math inline">\(\mathbf{K}'\)</span> is flipped both horizontally and vertically.</li>
<li>That is to say, when the convolutional layer performs strict <em>convolution</em> for the input in :numref:<code>fig_correlation</code> and <span class="math inline">\(\mathbf{K}'\)</span>, the same output in :numref:<code>fig_correlation</code> (cross-correlation of the input and <span class="math inline">\(\mathbf{K}\)</span>) will be obtained.</li>
<li>continue to refer to the cross-correlation operation as a convolution even though, strictly-speaking, it is slightly different.</li>
<li>Besides, we use the term <em>element</em> to refer to an entry (or component) of any tensor representing a layer representation or a convolution kernel.</li>
</ul>
</section>
<section id="feature-map-and-receptive-field" class="slide level2">
<h2>Feature Map and Receptive Field</h2>
<ul>
<li>As described in <em>Why Con Channels</em>, the convolutional layer output in :numref:<code>fig_correlation</code> is sometimes called a <em>feature map</em>, as it can be regarded as the learned representations (features) in the spatial dimensions (e.g., width and height) to the subsequent layer.</li>
<li>in CNNs, for any element <span class="math inline">\(x\)</span> of some layer, its <em>receptive field</em> refers to all the elements (from all the previous layers) that may affect the calculation of <span class="math inline">\(x\)</span> during the forward propagation.</li>
<li>receptive field may be larger than the actual size of the input.</li>
<li>continue to use :numref:<code>fig_correlation</code> to explain the receptive field. Given the <span class="math inline">\(2 \times 2\)</span> convolution kernel, the receptive field of the shaded output element (of value <span class="math inline">\(19\)</span>) is the four elements in the shaded portion of the input.</li>
<li>denote the <span class="math inline">\(2 \times 2\)</span> output as <span class="math inline">\(\mathbf{Y}\)</span> and consider a deeper CNN with an additional <span class="math inline">\(2 \times 2\)</span> convolutional layer that takes <span class="math inline">\(\mathbf{Y}\)</span> as its input, outputting a single element <span class="math inline">\(z\)</span>.</li>
<li>In this case, the receptive field of <span class="math inline">\(z\)</span> on <span class="math inline">\(\mathbf{Y}\)</span> includes all the four elements of <span class="math inline">\(\mathbf{Y}\)</span>, while the receptive field on the input includes all the nine input elements.</li>
<li>Thus, when any element in a feature map needs a larger receptive field to detect input features over a broader area, we can build a deeper network.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>Receptive fields derive their name from neurophysiology. In a series of experiments :cite:<code>Hubel.Wiesel.1959,Hubel.Wiesel.1962,Hubel.Wiesel.1968</code> on a range of animals and different stimuli, Hubel and iesel explored the response of what is called the visual cortex on said stimuli. By and large they found that lower levels respond to edges and related shapes. Later on, :citet:<code>Field.1987</code> illustrated this effect on natural images with, what can only be called, convolutional kernels.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>We reprint a key figure in :numref:<code>field_visual</code> to illustrate the striking similarities.</li>
</ul>

<img data-src="../img/field-visual.png" width="600" class="r-stretch quarto-figure-center"><p class="caption">Figure and caption taken from :citet:<code>Field.1987</code>: An example of coding with six different channels. (Left) Examples of the six types of sensor associated with each channel. (Right) Convolution of the image in (Middle) with the six sensors shown in (Left). The response of the individual sensors is determined by sampling these filtered images at a distance proportional to the size of the sensor (shown with dots). This diagram shows the response of only the even symmetric sensors.</p></section>
<section class="slide level2">

<ul>
<li>As it turns out, this relation even holds for the features computed by deeper layers of networks trained on image classification tasks, as demonstrated e.g., in :citet:<code>Kuzovkin.Vicente.Petton.ea.2018</code>. Suffice it to say, convolutions have proven to be an incredibly powerful tool for computer vision, both in biology and in code. As such, it is not surprising (in hindsight) that they heralded the recent success in deep learning.</li>
</ul>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li>core computation required for a convolutional layer is a cross-correlation operation. We saw that a simple nested for-loop is all that is required to compute its value.</li>
<li>If we have multiple input and multiple output channels, we are performing a matrix-matrix operation between channels.</li>
<li>As can be seen, the computation is straightforward and, most importantly, highly <em>local</em>.</li>
<li>This affords significant hardware optimization and many recent results in computer vision are only possible due to that. After all, it means that chip designers can invest into fast computation rather than memory, when it comes to optimizing for convolutions. While this may not lead to optimal designs for other applications, it opens the door to ubiquitous and affordable computer vision.</li>
<li>convolutions can be used for many purposes such as to detect edges and lines, to blur images, or to sharpen them.</li>
<li>Most importantly, it is not necessary that the statistician (or engineer) invents suitable filters.</li>
<li>Instead, we can simply <em>learn</em> them from data. This replaces feature engineering heuristics by evidence-based statistics.</li>
<li>filters are not just advantageous for building deep networks but also correspond to receptive fields and feature maps in the brain</li>
</ul>
</section>
<section id="exercises-1" class="slide level2">
<h2>Exercises</h2>
<ol type="1">
<li>Construct an image <code>X</code> with diagonal edges.
<ol type="1">
<li>What happens if you apply the kernel <code>K</code> in this section to it?</li>
<li>What happens if you transpose <code>X</code>?</li>
<li>What happens if you transpose <code>K</code>?</li>
</ol></li>
<li>Design some kernels manually.
<ol type="1">
<li>Given a directional vector <span class="math inline">\(\mathbf{v} = (v_1, v_2)\)</span>, derive an edge-detection kernel that detects edges orthogonal to <span class="math inline">\(\mathbf{v}\)</span>, i.e., edges in the direction <span class="math inline">\((v_2, -v_1)\)</span>.</li>
<li>Derive a finite difference operator for the second derivative. What is the minimum size of the convolutional kernel associate with it? Which structures in images respond most strongly to it?</li>
<li>How would you design a blur kernel? Why might you want to use such a kernel?</li>
<li>What is the minimum size of a kernel to obtain a derivative of order <span class="math inline">\(d\)</span>?</li>
</ol></li>
<li>When you try to automatically find the gradient for the <code>Conv2D</code> class we created, what kind of error message do you see?</li>
<li>How do you represent a cross-correlation operation as a matrix multiplication by changing the input and kernel tensors?</li>
</ol>
</section>
<section id="padding-and-stride" class="slide level2">
<h2>Padding and Stride</h2>
<ul>
<li>Recall the example of a convolution in :numref:<code>fig_correlation</code>. The input had both a height and width of 3 and the convolution kernel had both a height and width of 2, yielding an output representation with dimension <span class="math inline">\(2\times2\)</span>.</li>
<li>Assuming that the input shape is <span class="math inline">\(n_h\times n_w\)</span> and the convolution kernel shape is <span class="math inline">\(k_h\times k_w\)</span>, the output shape will be <span class="math inline">\((n_h-k_h+1) \times (n_w-k_w+1)\)</span>:<br>
</li>
<li>we can only shift the convolution kernel so far until it runs out of pixels to apply the convolution to.</li>
<li>In the following we will explore a number of techniques, including padding and strided convolutions, that offer more control over the size of the output.<br>
</li>
<li>As motivation, note that since kernels generally have width and height greater than <span class="math inline">\(1\)</span>, after applying many successive convolutions, we tend to wind up with outputs that are considerably smaller than our input. If we start with a <span class="math inline">\(240 \times 240\)</span> pixel image, <span class="math inline">\(10\)</span> layers of <span class="math inline">\(5 \times 5\)</span> convolutions reduce the image to <span class="math inline">\(200 \times 200\)</span> pixels, slicing off <span class="math inline">\(30 \%\)</span> of the image and with it obliterating any interesting information on the boundaries of the original image.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><em>Padding</em> most popular tool for handling this issue.</li>
<li>may want to reduce the dimensionality drastically, e.g., if we find the original input resolution to be unwieldy. <em>Strided convolutions</em> are a popular technique that can help in these instances.</li>
</ul>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1"></a><span class="im">import</span> torch</span>
<span id="cb17-2"><a href="#cb17-2"></a><span class="im">from</span> torch <span class="im">import</span> nn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="padding" class="slide level2">
<h2>Padding</h2>
<ul>
<li>As described above, one tricky issue when applying convolutional layers is that we tend to lose pixels on the perimeter of our image. Consider :numref:<code>img_conv_reuse</code> that depicts the pixel utilization as a function of the convolution kernel size and the position within the image. The pixels in the corners are hardly used at all.</li>
</ul>

<img data-src="../img/conv-reuse.svg" class="r-stretch quarto-figure-center"><p class="caption">Pixel utilization for convolutions of size <span class="math inline">\(1 \times 1\)</span>, <span class="math inline">\(2 \times 2\)</span>, and <span class="math inline">\(3 \times 3\)</span> respectively.</p></section>
<section class="slide level2">

<ul>
<li>typically use small kernels, for any given convolution, we might only lose a few pixels, but this can add up as we apply many successive convolutional layers.</li>
<li>one straightforward solution to this problem is to add extra pixels of filler around the boundary of our input image, thus increasing the effective size of the image.</li>
<li>Typically, we set the values of the extra pixels to zero. In :numref:<code>img_conv_pad</code>, we pad a <span class="math inline">\(3 \times 3\)</span> input, increasing its size to <span class="math inline">\(5 \times 5\)</span>.</li>
<li>The corresponding output then increases to a <span class="math inline">\(4 \times 4\)</span> matrix. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: <span class="math inline">\(0\times0+0\times1+0\times2+0\times3=0\)</span>.</li>
</ul>
</section>
<section class="slide level2">


<img data-src="../img/conv-pad.svg" width="500" class="r-stretch quarto-figure-center"><p class="caption">Two-dimensional cross-correlation with padding.</p><ul>
<li>In general, if we add a total of <span class="math inline">\(p_h\)</span> rows of padding (roughly half on top and half on bottom) and a total of <span class="math inline">\(p_w\)</span> columns of padding (roughly half on the left and half on the right), the output shape will be</li>
</ul>
<p><span class="math display">\[(n_h-k_h+p_h+1)\times(n_w-k_w+p_w+1).\]</span></p>
</section>
<section class="slide level2">

<ul>
<li><p>This means that the height and width of the output will increase by <span class="math inline">\(p_h\)</span> and <span class="math inline">\(p_w\)</span>, respectively.</p></li>
<li><p>In many cases, we will want to set <span class="math inline">\(p_h=k_h-1\)</span> and <span class="math inline">\(p_w=k_w-1\)</span> to give the input and output the same height and width. This will make it easier to predict the output shape of each layer when constructing the network. Assuming that <span class="math inline">\(k_h\)</span> is odd here, we will pad <span class="math inline">\(p_h/2\)</span> rows on both sides of the height. If <span class="math inline">\(k_h\)</span> is even, one possibility is to pad <span class="math inline">\(\lceil p_h/2\rceil\)</span> rows on the top of the input and <span class="math inline">\(\lfloor p_h/2\rfloor\)</span> rows on the bottom. We will pad both sides of the width in the same way.</p></li>
<li><p>CNNs commonly use convolution kernels with odd height and width values, such as 1, 3, 5, or 7. Choosing odd kernel sizes has the benefit that we can preserve the dimensionality while padding with the same number of rows on top and bottom, and the same number of columns on left and right.</p></li>
<li><p>practice of using odd kernels and padding to precisely preserve dimensionality offers a clerical benefit.</p></li>
<li><p>any two-dimensional tensor <code>X</code>, when the kernel’s size is odd and the number of adding rows and columns on all sides are the same, producing an output with the same height and width as the input, we know that the output <code>Y[i, j]</code> is calculated by cross-correlation of the input and convolution kernel with the window centered on <code>X[i, j]</code>.</p></li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>In the following example, we create a two-dimensional convolutional layer with a height and width of 3 and (<strong>apply 1 pixel of padding on all sides.</strong>)</li>
<li>input with a height and width of 8, we find that the height and width of the output is also 8.</li>
</ul>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="co">## We define a helper function to calculate convolutions. It initializes the</span></span>
<span id="cb18-2"><a href="#cb18-2"></a><span class="co">## convolutional layer weights and performs corresponding dimensionality</span></span>
<span id="cb18-3"><a href="#cb18-3"></a><span class="co">## elevations and reductions on the input and output</span></span>
<span id="cb18-4"><a href="#cb18-4"></a><span class="kw">def</span> comp_conv2d(conv2d, X):</span>
<span id="cb18-5"><a href="#cb18-5"></a>    <span class="co">## (1, 1) indicates that batch size and the number of channels are both 1</span></span>
<span id="cb18-6"><a href="#cb18-6"></a>    X <span class="op">=</span> X.reshape((<span class="dv">1</span>, <span class="dv">1</span>) <span class="op">+</span> X.shape)</span>
<span id="cb18-7"><a href="#cb18-7"></a>    Y <span class="op">=</span> conv2d(X)</span>
<span id="cb18-8"><a href="#cb18-8"></a>    <span class="co">## Strip the first two dimensions: examples and channels</span></span>
<span id="cb18-9"><a href="#cb18-9"></a>    <span class="cf">return</span> Y.reshape(Y.shape[<span class="dv">2</span>:])</span>
<span id="cb18-10"><a href="#cb18-10"></a></span>
<span id="cb18-11"><a href="#cb18-11"></a><span class="co">## 1 row and column is padded on either side, so a total of 2 rows or columns</span></span>
<span id="cb18-12"><a href="#cb18-12"></a><span class="co">## are added</span></span>
<span id="cb18-13"><a href="#cb18-13"></a>conv2d <span class="op">=</span> nn.LazyConv2d(<span class="dv">1</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-14"><a href="#cb18-14"></a>X <span class="op">=</span> torch.rand(size<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">8</span>))</span>
<span id="cb18-15"><a href="#cb18-15"></a>comp_conv2d(conv2d, X).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="50">
<pre><code>torch.Size([8, 8])</code></pre>
</div>
</div>
<ul>
<li>when the height and width of the convolution kernel are different, we can make the output and input have the same height and width by [<strong>setting different padding numbers for height and width.</strong>]</li>
</ul>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a><span class="co">## We use a convolution kernel with height 5 and width 3. The padding on either</span></span>
<span id="cb20-2"><a href="#cb20-2"></a><span class="co">## side of the height and width are 2 and 1, respectively</span></span>
<span id="cb20-3"><a href="#cb20-3"></a>conv2d <span class="op">=</span> nn.LazyConv2d(<span class="dv">1</span>, kernel_size<span class="op">=</span>(<span class="dv">5</span>, <span class="dv">3</span>), padding<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">1</span>))</span>
<span id="cb20-4"><a href="#cb20-4"></a>comp_conv2d(conv2d, X).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>torch.Size([8, 8])</code></pre>
</div>
</div>
</section>
<section id="stride" class="slide level2">
<h2>Stride</h2>
<ul>
<li>computing the cross-correlation, we start with the convolution window at the upper-left corner of the input tensor, then slide it over all locations both down and to the right.</li>
<li>previous: defaulted to sliding one element at a time.</li>
<li>sometimes, either for computational efficiency or because we wish to downsample, we move our window more than one element at a time, skipping the intermediate locations.</li>
<li>particularly useful if the convolution kernel is large since it captures a large area of the underlying image.</li>
<li>refer to the number of rows and columns traversed per slide as <em>stride</em>.</li>
<li>So far, we have used strides of 1, both for height and width. Sometimes, we may want to use a larger stride. :numref:<code>img_conv_stride</code> shows a two-dimensional cross-correlation operation with a stride of 3 vertically and 2 horizontally.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>The shaded portions are the output elements as well as the input and kernel tensor elements used for the output computation: <span class="math inline">\(0\times0+0\times1+1\times2+2\times3=8\)</span>, <span class="math inline">\(0\times0+6\times1+0\times2+0\times3=6\)</span>.</li>
<li>We can see that when the second element of the first column is generated, the convolution window slides down three rows.</li>
<li>The convolution window slides two columns to the right when the second element of the first row is generated.</li>
<li>When the convolution window continues to slide two columns to the right on the input, there is no output because the input element cannot fill the window (unless we add another column of padding).</li>
</ul>
</section>
<section class="slide level2">


<img data-src="../img/conv-stride.svg" width="300" class="r-stretch quarto-figure-center"><p class="caption">Cross-correlation with strides of 3 and 2 for height and width, respectively.</p><ul>
<li>In general, when the stride for the height is <span class="math inline">\(s_h\)</span> and the stride for the width is <span class="math inline">\(s_w\)</span>, the output shape is</li>
</ul>
<p><span class="math display">\[\lfloor(n_h-k_h+p_h+s_h)/s_h\rfloor \times \lfloor(n_w-k_w+p_w+s_w)/s_w\rfloor.\]</span></p>
<ul>
<li>If we set <span class="math inline">\(p_h=k_h-1\)</span> and <span class="math inline">\(p_w=k_w-1\)</span>, then the output shape can be simplified to <span class="math inline">\(\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor\)</span>.</li>
<li>Going a step further, if the input height and width are divisible by the strides on the height and width, then the output shape will be <span class="math inline">\((n_h/s_h) \times (n_w/s_w)\)</span>. Below, we [<strong>set the strides on both the height and width to 2</strong>], thus halving the input height and width.</li>
</ul>
</section>
<section class="slide level2">

<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a>conv2d <span class="op">=</span> nn.LazyConv2d(<span class="dv">1</span>, kernel_size<span class="op">=</span><span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb22-2"><a href="#cb22-2"></a>comp_conv2d(conv2d, X).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>torch.Size([4, 4])</code></pre>
</div>
</div>
<ul>
<li>Let’s look at (<strong>a slightly more complicated example</strong>).</li>
</ul>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a>conv2d <span class="op">=</span> nn.LazyConv2d(<span class="dv">1</span>, kernel_size<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">5</span>), padding<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>), stride<span class="op">=</span>(<span class="dv">3</span>, <span class="dv">4</span>))</span>
<span id="cb24-2"><a href="#cb24-2"></a>comp_conv2d(conv2d, X).shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>torch.Size([2, 2])</code></pre>
</div>
</div>
</section>
<section id="summary-and-discussion-1" class="slide level2">
<h2>Summary and Discussion</h2>
<ul>
<li>Padding can increase the height and width of the output. This is often used to give the output the same height and width as the input to avoid undesirable shrinkage of the output.</li>
<li>Moreover, it ensures that all pixels are used equally frequently.</li>
<li>Typically we pick symmetric padding on both sides of the input height and width. In this case we refer to <span class="math inline">\((p_h, p_w)\)</span> padding. Most commonly we set <span class="math inline">\(p_h = p_w\)</span>, in which case we simply state that we choose padding <span class="math inline">\(p\)</span>.</li>
<li>A similar convention applies to strides. When horizontal stride <span class="math inline">\(s_h\)</span> and vertical stride <span class="math inline">\(s_w\)</span> match, we simply talk about stride <span class="math inline">\(s\)</span>.</li>
<li>The stride can reduce the resolution of the output, for example reducing the height and width of the output to only <span class="math inline">\(1/n\)</span> of the height and width of the input for <span class="math inline">\(n &gt; 1\)</span>. By default, the padding is 0 and the stride is 1.</li>
<li>So far all padding that we discussed simply extended images with zeros.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>This has significant computational benefit since it is trivial to accomplish.</li>
<li>Moreover, operators can be engineered to take advantage of this padding implicitly without the need to allocate additional memory.</li>
<li>At the same time, it allows CNNs to encode implicit position information within an image, simply by learning where the “whitespace” is.</li>
<li>There are many alternatives to zero-padding. :citet:<code>Alsallakh.Kokhlikyan.Miglani.ea.2020</code> provided an extensive overview of alternatives (albeit without a clear case to use nonzero paddings unless artifacts occur).</li>
</ul>
</section>
<section id="exercises-2" class="slide level2">
<h2>Exercises</h2>
<ol type="1">
<li>Given the last code example in this section with kernel size <span class="math inline">\((3, 5)\)</span>, padding <span class="math inline">\((0, 1)\)</span>, and stride <span class="math inline">\((3, 4)\)</span>, calculate the output shape to check if it is consistent with the experimental result.</li>
<li>For audio signals, what does a stride of 2 correspond to?</li>
<li>Implement mirror padding, i.e., padding where the border values are simply mirrored to extend tensors.</li>
<li>What are the computational benefits of a stride larger than 1?</li>
<li>What might be statistical benefits of a stride larger than 1?</li>
<li>How would you implement a stride of <span class="math inline">\(\frac{1}{2}\)</span>? What does it correspond to? When would this be useful?</li>
</ol>
</section>
<section id="multiple-input-and-multiple-output-channels" class="slide level2">
<h2>Multiple Input and Multiple Output Channels</h2>
<ul>
<li>while we described the multiple channels that comprise each image (e.g., color images have the standard RGB channels to indicate the amount of red, green and blue) and convolutional layers for multiple channels in :numref:<code>subsec_why-conv-channels</code>, until now, we simplified all of our numerical examples by working with just a single input and a single output channel.</li>
<li>This allowed us to think of our inputs, convolution kernels, and outputs each as two-dimensional tensors.</li>
<li>When we add channels into the mix, our inputs and hidden representations both become three-dimensional tensors. For example, each RGB input image has shape <span class="math inline">\(3\times h\times w\)</span>. We refer to this axis, with a size of 3, as the <em>channel</em> dimension. The notion of channels is as old as CNNs themselves. For instance LeNet5 :cite:<code>LeCun.Jackel.Bottou.ea.1995</code> uses them. In this section, we will take a deeper look at convolution kernels with multiple input and multiple output channels.</li>
</ul>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a><span class="im">from</span> d2l <span class="im">import</span> torch <span class="im">as</span> d2l</span>
<span id="cb26-2"><a href="#cb26-2"></a><span class="im">import</span> torch</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="multiple-input-channels" class="slide level2">
<h2>Multiple Input Channels</h2>
<ul>
<li>When the input data contains multiple channels, we need to construct a convolution kernel with the same number of input channels as the input data, so that it can perform cross-correlation with the input data.</li>
<li>Assuming that the number of channels for the input data is <span class="math inline">\(c_i\)</span>, the number of input channels of the convolution kernel also needs to be <span class="math inline">\(c_i\)</span>. If our convolution kernel’s window shape is <span class="math inline">\(k_h\times k_w\)</span>, then when <span class="math inline">\(c_i=1\)</span>, we can think of our convolution kernel as just a two-dimensional tensor of shape <span class="math inline">\(k_h\times k_w\)</span>.</li>
<li>However, when <span class="math inline">\(c_i&gt;1\)</span>, we need a kernel that contains a tensor of shape <span class="math inline">\(k_h\times k_w\)</span> for <em>every</em> input channel. Concatenating these <span class="math inline">\(c_i\)</span> tensors together yields a convolution kernel of shape <span class="math inline">\(c_i\times _h\times k_w\)</span>.</li>
<li>Since the input and convolution kernel each have <span class="math inline">\(c_i\)</span> channels, we can perform a cross-correlation operation on the two-dimensional tensor of the input and the two-dimensional tensor of the convolution kernel for each channel, adding the <span class="math inline">\(c_i\)</span> results together (summing over the channels) to yield a two-dimensional tensor.</li>
<li>This is the result of a two-dimensional cross-correlation between a multi-channel input and a multi-input-channel convolution kernel.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>:numref:<code>fig_conv_multi_in</code> provides an example of a two-dimensional cross-correlation with two input channels. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation:</li>
<li><span class="math inline">\((1\times1+2\times2+4\times3+5\times4)+(0\times0+1\times1+3\times2+4\times3)=56\)</span>.</li>
</ul>
<p><img data-src="../img/conv-multi-in.svg" alt="Cross-correlation computation with 2 input channels."> :label:<code>fig_conv_multi_in</code></p>
<ul>
<li>To make sure we really understand what is going on here, we can (<strong>implement cross-correlation operations with multiple input channels</strong>) ourselves.</li>
<li>Notice that all we are doing is performing a cross-correlation operation per channel and then adding up the results.</li>
</ul>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1"></a><span class="kw">def</span> corr2d_multi_in(X, K):</span>
<span id="cb27-2"><a href="#cb27-2"></a>    <span class="co">## Iterate through the 0th dimension (channel) of K first, then add them up</span></span>
<span id="cb27-3"><a href="#cb27-3"></a>    <span class="cf">return</span> <span class="bu">sum</span>(d2l.corr2d(x, k) <span class="cf">for</span> x, k <span class="kw">in</span> <span class="bu">zip</span>(X, K))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section class="slide level2">

<ul>
<li>can construct the input tensor <code>X</code> and the kernel tensor <code>K</code> corresponding to the values in :numref:<code>fig_conv_multi_in</code> to (<strong>validate the output</strong>) of the cross-correlation operation.</li>
</ul>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a>X <span class="op">=</span> d2l.tensor([[[<span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">2.0</span>], [<span class="fl">3.0</span>, <span class="fl">4.0</span>, <span class="fl">5.0</span>], [<span class="fl">6.0</span>, <span class="fl">7.0</span>, <span class="fl">8.0</span>]],</span>
<span id="cb28-2"><a href="#cb28-2"></a>               [[<span class="fl">1.0</span>, <span class="fl">2.0</span>, <span class="fl">3.0</span>], [<span class="fl">4.0</span>, <span class="fl">5.0</span>, <span class="fl">6.0</span>], [<span class="fl">7.0</span>, <span class="fl">8.0</span>, <span class="fl">9.0</span>]]])</span>
<span id="cb28-3"><a href="#cb28-3"></a>K <span class="op">=</span> d2l.tensor([[[<span class="fl">0.0</span>, <span class="fl">1.0</span>], [<span class="fl">2.0</span>, <span class="fl">3.0</span>]], [[<span class="fl">1.0</span>, <span class="fl">2.0</span>], [<span class="fl">3.0</span>, <span class="fl">4.0</span>]]])</span>
<span id="cb28-4"><a href="#cb28-4"></a></span>
<span id="cb28-5"><a href="#cb28-5"></a>corr2d_multi_in(X, K)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="56">
<pre><code>tensor([[ 56.,  72.],
        [104., 120.]])</code></pre>
</div>
</div>
</section>
<section id="multiple-output-channels" class="slide level2">
<h2>Multiple Output Channels</h2>
<ul>
<li>Regardless of the number of input channels, so far we always ended up with one output channel. However, as we discussed in :numref:<code>subsec_why-conv-channels</code>, it turns out to be essential to have multiple channels at each layer. In the most popular neural network architectures, we actually increase the channel dimension as we go deeper in the neural network, typically downsampling to trade off spatial resolution for greater <em>channel depth</em>.</li>
<li>Intuitively, could think of each channel as responding to a different set of features.</li>
<li>reality is a bit more complicated than this. naive interpretation would suggest that representations are learned independently per pixel or per channel.</li>
<li>Instead, channels are optimized to be jointly useful. This means that rather than mapping a single channel to an edge detector, it may simply mean that some direction in channel space corresponds to detecting edges.</li>
<li>Denote by <span class="math inline">\(c_i\)</span> and <span class="math inline">\(c_o\)</span> the number of input and output channels, respectively, and let <span class="math inline">\(k_h\)</span> and <span class="math inline">\(k_w\)</span> be the height and width of the kernel. To get an output with multiple channels, we can create a kernel tensor of shape <span class="math inline">\(c_i\times k_h\times k_w\)</span> for <em>every</em> output channel.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>concatenate them on the output channel dimension, so that the shape of the convolution kernel is <span class="math inline">\(c_o\times c_i\times k_h\times k_w\)</span>. In cross-correlation operations, the result on each output channel is calculated from the convolution kernel corresponding to that output channel and takes input from all channels in the input tensor.</li>
<li>implement a cross-correlation function to [<strong>calculate the output of multiple channels</strong>] as shown below.</li>
</ul>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="kw">def</span> corr2d_multi_in_out(X, K):</span>
<span id="cb30-2"><a href="#cb30-2"></a>    <span class="co">## Iterate through the 0th dimension of K, and each time, perform</span></span>
<span id="cb30-3"><a href="#cb30-3"></a>    <span class="co">## cross-correlation operations with input X. All of the results are</span></span>
<span id="cb30-4"><a href="#cb30-4"></a>    <span class="co">## stacked together</span></span>
<span id="cb30-5"><a href="#cb30-5"></a>    <span class="cf">return</span> d2l.stack([corr2d_multi_in(X, k) <span class="cf">for</span> k <span class="kw">in</span> K], <span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>We construct a trivial convolution kernel with 3 output channels by concatenating the kernel tensor for <code>K</code> with <code>K+1</code> and <code>K+2</code>.</li>
</ul>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a>K <span class="op">=</span> d2l.stack((K, K <span class="op">+</span> <span class="dv">1</span>, K <span class="op">+</span> <span class="dv">2</span>), <span class="dv">0</span>)</span>
<span id="cb31-2"><a href="#cb31-2"></a>K.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="58">
<pre><code>torch.Size([3, 2, 2, 2])</code></pre>
</div>
</div>
</section>
<section class="slide level2">

<ul>
<li>Below, we perform cross-correlation operations on the input tensor <code>X</code> with the kernel tensor <code>K</code>. Now the output contains 3 channels. The result of the first channel is consistent with the result of the previous input tensor <code>X</code> and the multi-input channel, single-output channel kernel.</li>
</ul>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1"></a>corr2d_multi_in_out(X, K)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="59">
<pre><code>tensor([[[ 56.,  72.],
         [104., 120.]],

        [[ 76., 100.],
         [148., 172.]],

        [[ 96., 128.],
         [192., 224.]]])</code></pre>
</div>
</div>
</section>
<section id="times-1-convolutional-layer" class="slide level2">
<h2><span class="math inline">\(1\times 1\)</span> Convolutional Layer</h2>
<ul>
<li>At first, a [<strong><span class="math inline">\(1 \times 1\)</span> convolution</strong>], i.e., <span class="math inline">\(k_h = k_w = 1\)</span>, does not seem to make much sense.</li>
<li>After all, a convolution correlates adjacent pixels. A <span class="math inline">\(1 \times 1\)</span> convolution obviously does not. Nonetheless, they are popular operations that are sometimes included in the designs of complex deep networks :cite:<code>Lin.Chen.Yan.2013,Szegedy.Ioffe.Vanhoucke.ea.2017</code> Let’s see in some detail what it actually does.</li>
<li>Because the minimum window is used, the <span class="math inline">\(1\times 1\)</span> convolution loses the ability of larger convolutional layers to recognize patterns consisting of interactions among adjacent elements in the height and width dimensions. The only computation of the <span class="math inline">\(1\times 1\)</span> convolution occurs on the channel dimension.</li>
<li>:numref:<code>fig_conv_1x1</code> shows the cross-correlation computation using the <span class="math inline">\(1\times 1\)</span> convolution kernel with 3 input channels and 2 output channels.</li>
<li>Note that the inputs and outputs have the same height and width. Each element in the output is derived from a linear combination of elements <em>at the same position</em> in the input image.</li>
<li>You could think of the <span class="math inline">\(1\times 1\)</span> convolutional layer as constituting a fully connected layer applied at every single pixel location to transform the <span class="math inline">\(c_i\)</span> corresponding input values into <span class="math inline">\(c_o\)</span> output values.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>Because this is still a convolutional layer, the weights are tied across pixel location. Thus the <span class="math inline">\(1\times 1\)</span> convolutional layer requires <span class="math inline">\(c_o\times c_i\)</span> weights (plus the bias). Also note that convolutional layers are typically followed by nonlinearities. This ensures that <span class="math inline">\(1 \times 1\)</span> convolutions cannot simply be folded into other convolutions.</li>
</ul>

<img data-src="../img/conv-1x1.svg" class="r-stretch quarto-figure-center"><p class="caption">The cross-correlation computation uses the <span class="math inline">\(1\times 1\)</span> convolution kernel with 3 input channels and 2 output channels. The input and output have the same height and width.</p><ul>
<li>Let’s check whether this works in practice: we implement a <span class="math inline">\(1 \times 1\)</span> convolution using a fully connected layer.</li>
<li>The only thing is that we need to make some adjustments to the data shape before and after the matrix multiplication.</li>
</ul>
</section>
<section class="slide level2">

<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1"></a><span class="kw">def</span> corr2d_multi_in_out_1x1(X, K):</span>
<span id="cb35-2"><a href="#cb35-2"></a>    c_i, h, w <span class="op">=</span> X.shape</span>
<span id="cb35-3"><a href="#cb35-3"></a>    c_o <span class="op">=</span> K.shape[<span class="dv">0</span>]</span>
<span id="cb35-4"><a href="#cb35-4"></a>    X <span class="op">=</span> d2l.reshape(X, (c_i, h <span class="op">*</span> w))</span>
<span id="cb35-5"><a href="#cb35-5"></a>    K <span class="op">=</span> d2l.reshape(K, (c_o, c_i))</span>
<span id="cb35-6"><a href="#cb35-6"></a>    <span class="co">## Matrix multiplication in the fully connected layer</span></span>
<span id="cb35-7"><a href="#cb35-7"></a>    Y <span class="op">=</span> d2l.matmul(K, X)</span>
<span id="cb35-8"><a href="#cb35-8"></a>    <span class="cf">return</span> d2l.reshape(Y, (c_o, h, w))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>When performing <span class="math inline">\(1\times 1\)</span> convolutions, the above function is equivalent to the previously implemented cross-correlation function <code>corr2d_multi_in_out</code>. Let’s check this with some sample data.</li>
</ul>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1"></a>X <span class="op">=</span> d2l.normal(<span class="dv">0</span>, <span class="dv">1</span>, (<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">3</span>))</span>
<span id="cb36-2"><a href="#cb36-2"></a>K <span class="op">=</span> d2l.normal(<span class="dv">0</span>, <span class="dv">1</span>, (<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb36-3"><a href="#cb36-3"></a>Y1 <span class="op">=</span> corr2d_multi_in_out_1x1(X, K)</span>
<span id="cb36-4"><a href="#cb36-4"></a>Y2 <span class="op">=</span> corr2d_multi_in_out(X, K)</span>
<span id="cb36-5"><a href="#cb36-5"></a><span class="cf">assert</span> <span class="bu">float</span>(d2l.reduce_sum(d2l.<span class="bu">abs</span>(Y1 <span class="op">-</span> Y2))) <span class="op">&lt;</span> <span class="fl">1e-6</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="discussion" class="slide level2">
<h2>Discussion</h2>
<ul>
<li>Channels allow us to combine the best of both worlds: MLPs that allow for significant nonlinearities and convolutions that allow for <em>localized</em> analysis of features. In particular, channels allow the CNN to reason with multiple features, such as edge and shape detectors at the same time.</li>
<li>also offer a practical trade-off between the drastic parameter reduction arising from translation invariance and locality, and the need for expressive and diverse models in computer vision.</li>
<li>flexibility comes at a price. Given an image of size <span class="math inline">\((h \times w)\)</span>, the cost for computing a <span class="math inline">\(k \times k\)</span> convolution is <span class="math inline">\(\mathcal{O}(h \cdot w \cdot k^2)\)</span>. For <span class="math inline">\(c_i\)</span> and <span class="math inline">\(c_o\)</span> input and output channels respectively this increases to <span class="math inline">\(\mathcal{O}(h \cdot w \cdot k^2 \cdot c_i \cdot c_o)\)</span>.</li>
<li>For a <span class="math inline">\(256 \times 256\)</span> pixel image with a <span class="math inline">\(5 \times 5\)</span> kernel and <span class="math inline">\(128\)</span> input and output channels respectively</li>
<li>amounts to over 53 billion operations (we count multiplications and additions separately).</li>
<li>Later: effective strategies to cut down on the cost, e.g., by requiring the channel-wise operations to be block-diagonal, leading to architectures such as ResNeXt :cite:<code>Xie.Girshick.Dollar.ea.2017</code>.</li>
</ul>
</section>
<section id="exercises-3" class="slide level2">
<h2>Exercises</h2>
<ol type="1">
<li>Assume that we have two convolution kernels of size <span class="math inline">\(k_1\)</span> and <span class="math inline">\(k_2\)</span>, respectively (with no nonlinearity in-between).
<ol type="1">
<li>Prove that the result of the operation can be expressed by a single convolution.</li>
<li>What is the dimensionality of the equivalent single convolution?</li>
<li>Is the converse true, i.e., can you always decompose a convolution into two smaller ones?</li>
</ol></li>
<li>Assume an input of shape <span class="math inline">\(c_i\times h\times w\)</span> and a convolution kernel of shape <span class="math inline">\(c_o\times c_i\times k_h\times k_w\)</span>, padding of <span class="math inline">\((p_h, p_w)\)</span>, and stride of <span class="math inline">\((s_h, s_w)\)</span>.
<ol type="1">
<li>What is the computational cost (multiplications and additions) for the forward propagation?</li>
<li>What is the memory footprint?</li>
<li>What is the memory footprint for the backward computation?</li>
<li>What is the computational cost for the backpropagation?</li>
</ol></li>
<li>By what factor does the number of calculations increase if we double the number of input channels <span class="math inline">\(c_i\)</span> and the number of output channels <span class="math inline">\(c_o\)</span>? What happens if we double the padding?</li>
</ol>
</section>
<section class="slide level2">

<ol type="1">
<li>Are the variables <code>Y1</code> and <code>Y2</code> in the last example of this section exactly the same? Why?</li>
<li>Express convolutions as a matrix multiplication, even when the convolution window is not <span class="math inline">\(1 \times 1\)</span>?</li>
<li>Your task is to implement fast convolutions with a <span class="math inline">\(k \times k\)</span> kernel. One of the algorithm candidates is to scan horizontally across the source, reading a <span class="math inline">\(k\)</span>-wide strip and computing the <span class="math inline">\(1\)</span>-wide output strip one value at a time. The alternative is to read a <span class="math inline">\(k + \Delta\)</span> wide strip and compute a <span class="math inline">\(\Delta\)</span>-wide output strip. Why is the latter preferable? Is there a limit to how large you should choose <span class="math inline">\(\Delta\)</span>?</li>
<li>Assume that we have a <span class="math inline">\(c \times c\)</span> matrix.
<ol type="1">
<li>How much faster is it to multiply with a block-diagonal matrix if the matrix is broken up into <span class="math inline">\(b\)</span> blocks?</li>
<li>What is the downside of having <span class="math inline">\(b\)</span> blocks? How could you fix it, at least partly?</li>
</ol></li>
</ol>
</section>
<section id="pooling" class="slide level2">
<h2>Pooling</h2>
<ul>
<li>in many cases our ultimate task asks some global question about the image, e.g., <em>does it contain a cat?</em> Consequently, the units of our final layer should be sensitive to the entire input.</li>
<li>By gradually aggregating information, yielding coarser and coarser maps, we accomplish this goal of ultimately learning a global representation, while keeping all of the advantages of convolutional layers at the intermediate layers of processing.</li>
<li>The deeper we go in the network, the larger the receptive field (relative to the input) to which each hidden node is sensitive. Reducing spatial resolution accelerates this process, since the convolution kernels cover a larger effective area.</li>
<li>Moreover, when detecting lower-level features, such as edges (as discussed in :numref:<code>sec_conv_layer</code>), we often want our representations to be somewhat invariant to translation.</li>
<li>For instance, if we take the image <code>X</code> with a sharp delineation between black and white and shift the whole image by one pixel to the right, i.e., <code>Z[i, j] = X[i, j + 1]</code>, then the output for the new image <code>Z</code> might be vastly different.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>The edge will have shifted by one pixel. In reality, objects hardly ever occur exactly at the same place. In fact, even with a tripod and a stationary object, vibration of the camera due to the movement of the shutter might shift everything by a pixel or so (high-end cameras are loaded with special features to address this problem).</li>
<li>This section introduces <em>pooling layers</em>, which serve the dual purposes of mitigating the sensitivity of convolutional layers to location and of spatially downsampling representations.</li>
</ul>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1"></a><span class="im">from</span> d2l <span class="im">import</span> torch <span class="im">as</span> d2l</span>
<span id="cb37-2"><a href="#cb37-2"></a><span class="im">import</span> torch</span>
<span id="cb37-3"><a href="#cb37-3"></a><span class="im">from</span> torch <span class="im">import</span> nn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="maximum-pooling-and-average-pooling" class="slide level2">
<h2>Maximum Pooling and Average Pooling</h2>
<ul>
<li>Like convolutional layers, <em>pooling</em> operators consist of a fixed-shape window that is slid over all regions in the input according to its stride, computing a single output for each location traversed by the fixed-shape window (sometimes known as the <em>pooling window</em>).</li>
<li>However, unlike the cross-correlation computation of the inputs and kernels in the convolutional layer, the pooling layer contains no parameters (there is no <em>kernel</em>).</li>
<li>Instead, pooling operators are deterministic, typically calculating either the maximum or the average value of the elements in the pooling window.</li>
<li>These operations are called <em>maximum pooling</em> (<em>max-pooling</em> for short) and <em>average pooling</em>, respectively.</li>
<li><em>Average pooling</em> is essentially as old as CNNs. The idea is akin to downsampling an image.</li>
<li>Rather than just taking the value of every second (or third) pixel for the lower resolution image, we can average over adjacent pixels to obtain an image with better signal to noise ratio since we are combining the information from multiple adjacent pixels.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li><em>Max-pooling</em> was introduced in :citet:<code>Riesenhuber.Poggio.1999</code> in the context of cognitive neuroscience to describe how information aggregation might be aggregated hierarchically for the purpose of object recognition, and an earlier version in speech recognition :cite:<code>Yamaguchi.Sakamoto.Akabane.ea.1990</code>.</li>
<li>In almost all cases, max-pooling, as it is also referred to, is preferable.</li>
<li>In both cases, as with the cross-correlation operator, we can think of the pooling window as starting from the upper-left of the input tensor and sliding across the input tensor from left to right and top to bottom.</li>
<li>At each location that the pooling window hits, it computes the maximum or average value of the input subtensor in the window, depending on whether max or average pooling is employed.</li>
</ul>

<img data-src="../img/pooling.svg" width="300" class="r-stretch quarto-figure-center"><p class="caption">Max-pooling with a pooling window shape of <span class="math inline">\(2\times 2\)</span>. The shaded portions are the first output element as well as the input tensor elements used for the output computation: <span class="math inline">\(\max(0, 1, 3, 4)=4\)</span>.</p></section>
<section class="slide level2">

<ul>
<li>output tensor in :numref:<code>fig_pooling</code> has a height of 2 and a width of 2. The four elements are derived from the maximum value in each pooling window:</li>
</ul>
<p><span class="math display">\[
\max(0, 1, 3, 4)=4,\\
\max(1, 2, 4, 5)=5,\\
\max(3, 4, 6, 7)=7,\\
\max(4, 5, 7, 8)=8.\\
\]</span></p>
</section>
<section class="slide level2">

<ul>
<li>More generally, we can define a <span class="math inline">\(p \times q\)</span> pooling layer by aggregating over a region of said size. Returning to the problem of edge detection, we use the output of the convolutional layer as input for <span class="math inline">\(2\times 2\)</span> max-pooling.</li>
<li>Denote by <code>X</code> the input of the convolutional layer input and <code>Y</code> the pooling layer output. Regardless of whether or not the values of <code>X[i, j]</code>, <code>X[i, j + 1]</code>, <code>X[i+1, j]</code> and <code>X[i+1, j + 1]</code> are different, the pooling layer always outputs <code>Y[i, j] = 1</code>.</li>
<li>That is to say, using the <span class="math inline">\(2\times 2\)</span> max-pooling layer, we can still detect if the pattern recognized by the convolutional layer moves no more than one element in height or width.</li>
<li>In the code below, we (<strong>implement the forward propagation of the pooling layer</strong>) in the <code>pool2d</code> function. This function is similar to the <code>corr2d</code> function in :numref:<code>sec_conv_layer</code>.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>However, no kernel is needed, computing the output as either the maximum or the average of each region in the input.</li>
</ul>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a><span class="kw">def</span> pool2d(X, pool_size, mode<span class="op">=</span><span class="st">'max'</span>):</span>
<span id="cb38-2"><a href="#cb38-2"></a>    p_h, p_w <span class="op">=</span> pool_size</span>
<span id="cb38-3"><a href="#cb38-3"></a>    Y <span class="op">=</span> d2l.zeros((X.shape[<span class="dv">0</span>] <span class="op">-</span> p_h <span class="op">+</span> <span class="dv">1</span>, X.shape[<span class="dv">1</span>] <span class="op">-</span> p_w <span class="op">+</span> <span class="dv">1</span>))</span>
<span id="cb38-4"><a href="#cb38-4"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(Y.shape[<span class="dv">0</span>]):</span>
<span id="cb38-5"><a href="#cb38-5"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(Y.shape[<span class="dv">1</span>]):</span>
<span id="cb38-6"><a href="#cb38-6"></a>            <span class="cf">if</span> mode <span class="op">==</span> <span class="st">'max'</span>:</span>
<span id="cb38-7"><a href="#cb38-7"></a>                Y[i, j] <span class="op">=</span> X[i: i <span class="op">+</span> p_h, j: j <span class="op">+</span> p_w].<span class="bu">max</span>()</span>
<span id="cb38-8"><a href="#cb38-8"></a>            <span class="cf">elif</span> mode <span class="op">==</span> <span class="st">'avg'</span>:</span>
<span id="cb38-9"><a href="#cb38-9"></a>                Y[i, j] <span class="op">=</span> X[i: i <span class="op">+</span> p_h, j: j <span class="op">+</span> p_w].mean()</span>
<span id="cb38-10"><a href="#cb38-10"></a>    <span class="cf">return</span> Y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>can construct the input tensor <code>X</code> in :numref:<code>fig_pooling</code> to [<strong>validate the output of the two-dimensional max-pooling layer</strong>].</li>
</ul>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1"></a>X <span class="op">=</span> d2l.tensor([[<span class="fl">0.0</span>, <span class="fl">1.0</span>, <span class="fl">2.0</span>], [<span class="fl">3.0</span>, <span class="fl">4.0</span>, <span class="fl">5.0</span>], [<span class="fl">6.0</span>, <span class="fl">7.0</span>, <span class="fl">8.0</span>]])</span>
<span id="cb39-2"><a href="#cb39-2"></a>pool2d(X, (<span class="dv">2</span>, <span class="dv">2</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="64">
<pre><code>tensor([[4., 5.],
        [7., 8.]])</code></pre>
</div>
</div>
<ul>
<li>Also, we experiment with (<strong>the average pooling layer</strong>).</li>
</ul>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1"></a>pool2d(X, (<span class="dv">2</span>, <span class="dv">2</span>), <span class="st">'avg'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="65">
<pre><code>tensor([[2., 3.],
        [5., 6.]])</code></pre>
</div>
</div>
</section>
<section id="padding-and-stride-1" class="slide level2">
<h2>Padding and Stride</h2>
<ul>
<li>As with convolutional layers, pooling layers change the output shape. And as before, we can adjust the operation to achieve a desired output shape by padding the input and adjusting the stride.</li>
<li>We can demonstrate the use of padding and strides in pooling layers via the built-in two-dimensional max-pooling layer from the deep learning framework.</li>
<li>We first construct an input tensor <code>X</code> whose shape has four dimensions, where the number of examples (batch size) and number of channels are both 1.</li>
</ul>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1"></a>X <span class="op">=</span> d2l.reshape(d2l.arange(<span class="dv">16</span>, dtype<span class="op">=</span>d2l.float32), (<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">4</span>))</span>
<span id="cb43-2"><a href="#cb43-2"></a>X</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="66">
<pre><code>tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]]]])</code></pre>
</div>
</div>
</section>
<section class="slide level2">

<ul>
<li>pooling aggregates information from an area, (<strong>deep learning frameworks default to matching pooling window sizes and stride.</strong>) For instance, if we use a pooling window of shape <code>(3, 3)</code> we get a stride shape of <code>(3, 3)</code> by default.</li>
</ul>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1"></a>pool2d <span class="op">=</span> nn.MaxPool2d(<span class="dv">3</span>)</span>
<span id="cb45-2"><a href="#cb45-2"></a><span class="co">## Pooling has no model parameters, hence it needs no initialization</span></span>
<span id="cb45-3"><a href="#cb45-3"></a>pool2d(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="67">
<pre><code>tensor([[[[10.]]]])</code></pre>
</div>
</div>
<ul>
<li>as expected, [<strong>the stride and padding can be manually specified</strong>] to override framework defaults if needed.</li>
</ul>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1"></a>pool2d <span class="op">=</span> nn.MaxPool2d(<span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb47-2"><a href="#cb47-2"></a>pool2d(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="68">
<pre><code>tensor([[[[ 5.,  7.],
          [13., 15.]]]])</code></pre>
</div>
</div>
</section>
<section class="slide level2">

<p>Of course, we can specify an arbitrary rectangular pooling window with arbitrary height and width respectively, as the example below shows.</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1"></a>pool2d <span class="op">=</span> nn.MaxPool2d((<span class="dv">2</span>, <span class="dv">3</span>), stride<span class="op">=</span>(<span class="dv">2</span>, <span class="dv">3</span>), padding<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb49-2"><a href="#cb49-2"></a>pool2d(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="69">
<pre><code>tensor([[[[ 5.,  7.],
          [13., 15.]]]])</code></pre>
</div>
</div>
</section>
<section id="multiple-channels" class="slide level2">
<h2>Multiple Channels</h2>
<ul>
<li>When processing multi-channel input data, [<strong>the pooling layer pools each input channel separately</strong>], rather than summing the inputs up over channels as in a convolutional layer.</li>
<li>This means that the number of output channels for the pooling layer is the same as the number of input channels. Below, we will concatenate tensors <code>X</code> and <code>X + 1</code> on the channel dimension to construct an input with 2 channels.</li>
</ul>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1"></a>X <span class="op">=</span> d2l.concat((X, X <span class="op">+</span> <span class="dv">1</span>), <span class="dv">1</span>)</span>
<span id="cb51-2"><a href="#cb51-2"></a>X</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="70">
<pre><code>tensor([[[[ 0.,  1.,  2.,  3.],
          [ 4.,  5.,  6.,  7.],
          [ 8.,  9., 10., 11.],
          [12., 13., 14., 15.]],

         [[ 1.,  2.,  3.,  4.],
          [ 5.,  6.,  7.,  8.],
          [ 9., 10., 11., 12.],
          [13., 14., 15., 16.]]]])</code></pre>
</div>
</div>
</section>
<section class="slide level2">

<p>As we can see, the number of output channels is still 2 after pooling.</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1"></a>pool2d <span class="op">=</span> nn.MaxPool2d(<span class="dv">3</span>, padding<span class="op">=</span><span class="dv">1</span>, stride<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb53-2"><a href="#cb53-2"></a>pool2d(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="71">
<pre><code>tensor([[[[ 5.,  7.],
          [13., 15.]],

         [[ 6.,  8.],
          [14., 16.]]]])</code></pre>
</div>
</div>
</section>
<section id="summary-1" class="slide level2">
<h2>Summary</h2>
<ul>
<li>Pooling is an exceedingly simple operation. It does exactly what its name indicates, aggregate results over a window of values.</li>
<li>All convolution semantics, such as strides and padding apply in the same way as they did previously.</li>
<li>Note that pooling is indifferent to channels, i.e., it leaves the number of channels unchanged and it applies to each channel separately. Lastly, of the two popular pooling choices, max-pooling is preferable to average pooling, as it confers some degree of invariance to output. A popular choice is to pick a pooling window size of <span class="math inline">\(2 \times 2\)</span> to quarter the spatial resolution of output.</li>
<li>Note that there are many more ways of reducing resolution beyond pooling. For instance, in stochastic pooling :cite:<code>Zeiler.Fergus.2013</code> and fractional max-pooling :cite:<code>Graham.2014</code> aggregation is combined with randomization.</li>
<li>This can slightly improve the accuracy in some cases. Lastly, as we will see later with the attention mechanism, there are more refined ways of aggregating over outputs, e.g., by using the alignment between a query and representation vectors.</li>
</ul>
</section>
<section id="exercises-4" class="slide level2">
<h2>Exercises</h2>
<ol type="1">
<li>Implement average pooling through a convolution.</li>
<li>Prove that max-pooling cannot be implemented through a convolution alone.</li>
<li>Max-pooling can be accomplished using ReLU operations, i.e., <span class="math inline">\(\mathrm{ReLU}(x) = \max(0, x)\)</span>.
<ol type="1">
<li>Express <span class="math inline">\(\max (a, b)\)</span> by using only ReLU operations.</li>
<li>Use this to implement max-pooling by means of convolutions and ReLU layers.</li>
<li>How many channels and layers do you need for a <span class="math inline">\(2 \times 2\)</span> convolution? How many for a <span class="math inline">\(3 \times 3\)</span> convolution.</li>
</ol></li>
</ol>
</section>
<section class="slide level2">

<ol start="2" type="1">
<li>What is the computational cost of the pooling layer? Assume that the input to the pooling layer is of size <span class="math inline">\(c\times h\times w\)</span>, the pooling window has a shape of <span class="math inline">\(p_h\times p_w\)</span> with a padding of <span class="math inline">\((p_h, p_w)\)</span> and a stride of <span class="math inline">\((s_h, s_w)\)</span>.</li>
<li>Why do you expect max-pooling and average pooling to work differently?</li>
<li>Do we need a separate minimum pooling layer? Can you replace it with another operation?</li>
<li>We could use the softmax operation for pooling. Why might it not be so popular?</li>
</ol>
</section>
<section id="convolutional-neural-networks-lenet" class="slide level2">
<h2>Convolutional Neural Networks (LeNet)</h2>
<ul>
<li>We now have all the ingredients required to assemble a fully-functional CNN. In our earlier encounter with image data, we applied a linear model with softmax regression (:numref:<code>sec_softmax_scratch</code>) and an MLP (:numref:<code>sec_mlp-implementation</code>) to pictures of clothing in the Fashion-MNIST dataset. To make such data amenable we first flattened each image from a <span class="math inline">\(28\times28\)</span> matrix into a fixed-length <span class="math inline">\(784\)</span>-dimensional vector, and thereafter processed them in fully connected layers. Now that we have a handle on convolutional layers, we can retain the spatial structure in our images. As an additional benefit of replacing fully connected layers with convolutional layers, we will enjoy more parsimonious models that require far fewer parameters.</li>
<li>In this section, we will introduce <em>LeNet</em>, among the first published CNNs to capture wide attention for its performance on computer vision tasks. The model was introduced by (and named for) Yann LeCun, then a researcher at AT&amp;T Bell Labs, for the purpose of recognizing handwritten digits in images :cite:<code>LeCun.Bottou.Bengio.ea.1998</code>. This work represented the culmination of a decade of research developing the technology.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>In 1989, LeCun’s team published the first study to successfully train CNNs via backpropagation :cite:<code>LeCun.Boser.Denker.ea.1989</code>.</li>
<li>At the time LeNet achieved outstanding results matching the performance of support vector machines, then a dominant approach in supervised learning, achieving an error rate of less than 1% per digit.</li>
<li>LeNet was eventually adapted to recognize digits for processing deposits in ATM machines. To this day, some ATMs still run the code that Yann LeCun and his colleague Leon Bottou wrote in the 1990s!</li>
</ul>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1"></a><span class="im">from</span> d2l <span class="im">import</span> torch <span class="im">as</span> d2l</span>
<span id="cb55-2"><a href="#cb55-2"></a><span class="im">import</span> torch</span>
<span id="cb55-3"><a href="#cb55-3"></a><span class="im">from</span> torch <span class="im">import</span> nn</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="lenet" class="slide level2">
<h2>LeNet</h2>
<ul>
<li>LeNet (LeNet-5) consists of two parts:</li>
<li><ol type="i">
<li>a convolutional encoder consisting of two convolutional layers; and</li>
</ol></li>
<li><ol start="2" type="i">
<li>a dense block consisting of three fully connected layers</li>
</ol></li>
<li>The architecture is summarized in :numref:<code>img_lenet</code>.</li>
</ul>

<img data-src="../img/lenet.svg" data-center="True" width="600" class="r-stretch quarto-figure-center"><p class="caption">Data flow in LeNet. The input is a handwritten digit, the output a probability over 10 possible outcomes.</p></section>
<section class="slide level2">

<ul>
<li>The basic units in each convolutional block are a convolutional layer, a sigmoid activation function, and a subsequent average pooling operation. Note that while ReLUs and max-pooling work better, these discoveries had not yet been made at the time. Each convolutional layer uses a <span class="math inline">\(5\times 5\)</span> kernel and a sigmoid activation function. These layers map spatially arranged inputs to a number of two-dimensional feature maps, typically increasing the number of channels. The first convolutional layer has 6 output channels, while the second has 16. Each <span class="math inline">\(2\times2\)</span> pooling operation (stride 2) reduces dimensionality by a factor of <span class="math inline">\(4\)</span> via spatial downsampling. The convolutional block emits an output with shape given by (batch size, number of channel, height, width).</li>
<li>In order to pass output from the convolutional block to the dense block, we must flatten each example in the minibatch. In other words, we take this four-dimensional input and transform it into the two-dimensional input expected by fully connected layers:</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>as a reminder, the two-dimensional representation that we desire uses the first dimension to index examples in the minibatch and the second to give the flat vector representation of each example.</li>
<li>LeNet’s dense block has three fully connected layers, with 120, 84, and 10 outputs, respectively.</li>
<li>Because we are still performing classification, the 10-dimensional output layer corresponds to the number of possible output classes.</li>
<li>While getting to the point where you truly understand what is going on inside LeNet may have taken a bit of work, hopefully the following code snippet will convince you that implementing such models with modern deep learning frameworks is remarkably simple. We need only to instantiate a <code>Sequential</code> block and chain together the appropriate layers, using Xavier initialization as introduced in :numref:<code>subsec_xavier</code>.</li>
</ul>
</section>
<section class="slide level2">

<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1"></a><span class="kw">def</span> init_cnn(module):  <span class="co">#@save</span></span>
<span id="cb56-2"><a href="#cb56-2"></a>    <span class="co">"""Initialize weights for CNNs."""</span></span>
<span id="cb56-3"><a href="#cb56-3"></a>    <span class="cf">if</span> <span class="bu">type</span>(module) <span class="op">==</span> nn.Linear <span class="kw">or</span> <span class="bu">type</span>(module) <span class="op">==</span> nn.Conv2d:</span>
<span id="cb56-4"><a href="#cb56-4"></a>        nn.init.xavier_uniform_(module.weight)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1"></a><span class="kw">class</span> LeNet(d2l.Classifier):  <span class="co">#@save</span></span>
<span id="cb57-2"><a href="#cb57-2"></a>    <span class="co">"""The LeNet-5 model."""</span></span>
<span id="cb57-3"><a href="#cb57-3"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, lr<span class="op">=</span><span class="fl">0.1</span>, num_classes<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb57-4"><a href="#cb57-4"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb57-5"><a href="#cb57-5"></a>        <span class="va">self</span>.save_hyperparameters()</span>
<span id="cb57-6"><a href="#cb57-6"></a>        <span class="va">self</span>.net <span class="op">=</span> nn.Sequential(</span>
<span id="cb57-7"><a href="#cb57-7"></a>            nn.LazyConv2d(<span class="dv">6</span>, kernel_size<span class="op">=</span><span class="dv">5</span>, padding<span class="op">=</span><span class="dv">2</span>), nn.Sigmoid(),</span>
<span id="cb57-8"><a href="#cb57-8"></a>            nn.AvgPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb57-9"><a href="#cb57-9"></a>            nn.LazyConv2d(<span class="dv">16</span>, kernel_size<span class="op">=</span><span class="dv">5</span>), nn.Sigmoid(),</span>
<span id="cb57-10"><a href="#cb57-10"></a>            nn.AvgPool2d(kernel_size<span class="op">=</span><span class="dv">2</span>, stride<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb57-11"><a href="#cb57-11"></a>            nn.Flatten(),</span>
<span id="cb57-12"><a href="#cb57-12"></a>            nn.LazyLinear(<span class="dv">120</span>), nn.Sigmoid(),</span>
<span id="cb57-13"><a href="#cb57-13"></a>            nn.LazyLinear(<span class="dv">84</span>), nn.Sigmoid(),</span>
<span id="cb57-14"><a href="#cb57-14"></a>            nn.LazyLinear(num_classes))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li>take some liberty in the reproduction of LeNet insofar as we replace the Gaussian activation layer by a softmax layer. This greatly simplifies the implementation, not the least due to the fact that the Gaussian decoder is rarely used nowadays. Other than that, this network matches the original LeNet-5 architecture.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>Let’s see what happens inside the network. By passing a single-channel (black and white) <span class="math inline">\(28 \times 28\)</span> image through the network and printing the output shape at each layer, we can [<strong>inspect the model</strong>] to make sure that its operations line up with what we expect from :numref:<code>img_lenet_vert</code>.</li>
</ul>
<p><img data-src="../img/lenet-vert.svg" alt="Compressed notation for LeNet-5."> :label:<code>img_lenet_vert</code></p>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1"></a><span class="at">@d2l.add_to_class</span>(d2l.Classifier)  <span class="co">#@save</span></span>
<span id="cb58-2"><a href="#cb58-2"></a><span class="kw">def</span> layer_summary(<span class="va">self</span>, X_shape):</span>
<span id="cb58-3"><a href="#cb58-3"></a>    X <span class="op">=</span> d2l.randn(<span class="op">*</span>X_shape)</span>
<span id="cb58-4"><a href="#cb58-4"></a>    <span class="cf">for</span> layer <span class="kw">in</span> <span class="va">self</span>.net:</span>
<span id="cb58-5"><a href="#cb58-5"></a>        X <span class="op">=</span> layer(X)</span>
<span id="cb58-6"><a href="#cb58-6"></a>        <span class="bu">print</span>(layer.__class__.<span class="va">__name__</span>, <span class="st">'output shape:</span><span class="ch">\t</span><span class="st">'</span>, X.shape)</span>
<span id="cb58-7"><a href="#cb58-7"></a>        </span>
<span id="cb58-8"><a href="#cb58-8"></a>model <span class="op">=</span> LeNet()</span>
<span id="cb58-9"><a href="#cb58-9"></a>model.layer_summary((<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">28</span>, <span class="dv">28</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Conv2d output shape:     torch.Size([1, 6, 28, 28])
Sigmoid output shape:    torch.Size([1, 6, 28, 28])
AvgPool2d output shape:  torch.Size([1, 6, 14, 14])
Conv2d output shape:     torch.Size([1, 16, 10, 10])
Sigmoid output shape:    torch.Size([1, 16, 10, 10])
AvgPool2d output shape:  torch.Size([1, 16, 5, 5])
Flatten output shape:    torch.Size([1, 400])
Linear output shape:     torch.Size([1, 120])
Sigmoid output shape:    torch.Size([1, 120])
Linear output shape:     torch.Size([1, 84])
Sigmoid output shape:    torch.Size([1, 84])
Linear output shape:     torch.Size([1, 10])</code></pre>
</div>
</div>
</section>
<section class="slide level2">

<ul>
<li>height and width of the representation at each layer throughout the convolutional block is reduced (compared with the previous layer).</li>
<li>The first convolutional layer uses 2 pixels of padding to compensate for the reduction in height and width that would otherwise result from using a <span class="math inline">\(5 \times 5\)</span> kernel.</li>
<li>As an aside, the image size of <span class="math inline">\(28 \times 28\)</span> pixels in the original MNIST OCR dataset is a result of <em>trimming</em> 2 pixel rows (and columns) from the original scans that measured <span class="math inline">\(32 \times 32\)</span> pixels.</li>
<li>done primarily to save space (a 30% reduction) at a time when Megabytes mattered.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>In contrast, the second convolutional layer forgoes padding, and thus the height and width are both reduced by 4 pixels.</li>
<li>As we go up the stack of layers, the number of channels increases layer-over-layer from 1 in the input to 6 after the first convolutional layer and 16 after the second convolutional layer.</li>
<li>However, each pooling layer halves the height and width. Finally, each fully connected layer reduces dimensionality, finally emitting an output whose dimension matches the number of classes.</li>
</ul>
</section>
<section id="training" class="slide level2">
<h2>Training</h2>
<ul>
<li>Now that we have implemented the model, let’s [<strong>run an experiment to see how the LeNet-5 model fares on Fashion-MNIST</strong>].</li>
<li>While CNNs have fewer parameters, they can still be more expensive to compute than similarly deep MLPs because each parameter participates in many more multiplications.</li>
<li>If you have access to a GPU, this might be a good time to put it into action to speed up training.</li>
<li>Note that the <code>d2l.Trainer</code> class takes care of all details. By default, it initializes the model parameters on the available devices.</li>
</ul>
</section>
<section class="slide level2">

<ul>
<li>Just as with MLPs, our loss function is cross-entropy, and we minimize it via minibatch stochastic gradient descent.</li>
</ul>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1"></a>trainer <span class="op">=</span> d2l.Trainer(max_epochs<span class="op">=</span><span class="dv">10</span>, num_gpus<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb60-2"><a href="#cb60-2"></a>data <span class="op">=</span> d2l.FashionMNIST(batch_size<span class="op">=</span><span class="dv">128</span>)</span>
<span id="cb60-3"><a href="#cb60-3"></a>model <span class="op">=</span> LeNet(lr<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb60-4"><a href="#cb60-4"></a>model.apply_init([<span class="bu">next</span>(<span class="bu">iter</span>(data.get_dataloader(<span class="va">True</span>)))[<span class="dv">0</span>]], init_cnn)</span>
<span id="cb60-5"><a href="#cb60-5"></a>trainer.fit(model, data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>

</div>
<img data-src="04_convnets_0_files/figure-revealjs/cell-39-output-1.svg" class="r-stretch"></section>
<section id="summary-2" class="slide level2">
<h2>Summary</h2>
<ul>
<li>moved from the MLPs of the 1980s to the CNNs of the 1990s and early 2000s. The architectures proposed, e.g., in the form of LeNet-5 remain meaningful, even to this day. It is worth comparing the error rates on Fashion-MNIST achievable with LeNet-5 both to the very best possible with MLPs (:numref:<code>sec_mlp-implementation</code>) and those with significantly more advanced architectures such as ResNet (:numref:<code>sec_resnet</code>). LeNet is much more similar to the latter than to the former. One of the primary differences, as we shall see, is that greater amounts of computation afforded significantly more complex architectures.</li>
<li>second difference is the relative ease with which we were able to implement LeNet. What used to be an engineering challenge worth months of C++ and assembly code, engineering to improve SN, an early Lisp based deep learning tool :cite:<code>Bottou.Le-Cun.1988</code>, and finally experimentation with models can now be accomplished in minutes. It is this incredible productivity boost that has democratized deep learning model development tremendously. In the next chapter we will follow down this rabbit to hole to see where it takes us.</li>
</ul>
</section>
<section id="exercises-5" class="slide level2">
<h2>Exercises</h2>
<ol type="1">
<li>Let’s modernize LeNet. Implement and test the following changes:
<ol type="1">
<li>Replace the average pooling with max-pooling.</li>
<li>Replace the softmax layer with ReLU.</li>
</ol></li>
<li>Try to change the size of the LeNet style network to improve its accuracy in addition to max-pooling and ReLU.
<ol type="1">
<li>Adjust the convolution window size.</li>
<li>Adjust the number of output channels.</li>
<li>Adjust the number of convolution layers.</li>
<li>Adjust the number of fully connected layers.</li>
<li>Adjust the learning rates and other training details (e.g., initialization and number of epochs.)</li>
</ol></li>
</ol>
</section>
<section class="slide level2">

<ol start="3" type="1">
<li>Try out the improved network on the original MNIST dataset.</li>
<li>Display the activations of the first and second layer of LeNet for different inputs (e.g., sweaters and coats).</li>
<li>What happens to the activations when you feed significantly different images into the network (e.g., cats, cars, or even random noise)?</li>
</ol>

<img src="eclipse_logo_small.png" class="slide-logo r-stretch"><div class="footer footer-default">
<p><a href="https://github.com/ECLIPSE-Lab/SS24_DataScienceForEM">SS24_DataScienceForEM</a></p>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="04_convnets_0_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="04_convnets_0_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="04_convnets_0_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="04_convnets_0_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="04_convnets_0_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="04_convnets_0_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="04_convnets_0_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="04_convnets_0_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="04_convnets_0_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="04_convnets_0_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1920,

        height: 1080,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        text: function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>