{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "format: \n",
        "  revealjs:\n",
        "    theme: \"night\" #[\"theme/q-theme.scss\"]\n",
        "    slide-number: c/t\n",
        "    logo: \"eclipse_logo_small.png\"\n",
        "    footer: \"[SS24_DataScienceForEM](https://github.com/ECLIPSE-Lab/SS24_DataScienceForEM)\"\n",
        "    code-copy: true\n",
        "    center-title-slide: false\n",
        "    include-in-header: ../heading-meta.html\n",
        "    code-link: true\n",
        "    code-overflow: wrap\n",
        "    highlight-style: a11y\n",
        "    height: 1080\n",
        "    width: 1920\n",
        "    ## output-file: 02_preliminaries.html\n",
        "execute: \n",
        "  eval: true\n",
        "  echo: true\n",
        "---"
      ],
      "id": "1d1a3e84"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convolutional Neural Networks 0\n",
        "<br>\n",
        "<h2> Data Science in Electron Microscopy </h2>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<h3> Philipp Pelz </h3>\n",
        "\n",
        "<h3> 2024 </h3>\n",
        "<br>\n",
        "\n",
        "<h3>  &nbsp; [https://github.com/ECLIPSE-Lab/SS24_DataScienceForEM](https://github.com/ECLIPSE-Lab/SS24_DataScienceForEM)\n",
        "</h3>\n",
        "\n",
        "## From Fully Connected Layers to Convolutions\n",
        ":label:`sec_why-conv`\n",
        "\n",
        "- models that we have discussed so far remain appropriate options when we are dealing with tabular data.\n",
        "- tabular: data consist of rows corresponding to examples and columns corresponding to features. \n",
        "- With tabular data, we might anticipate that the patterns we seek could involve interactions among the features, but we do not assume any structure *a priori* concerning how the features interact.\n",
        "- no prior knowledge about structure of data --> MLP\n",
        "- for high-dimensional perceptual data, structure-less networks can grow unwieldy."
      ],
      "id": "4264b81b"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "--- \n",
        "\n",
        "- let's return to our running exampleof distinguishing cats from dogs.\n",
        "- Say that we do a thorough job in data collection, collecting an annotated dataset of one-megapixel photographs. \n",
        "- $\\rightarrow$ each input to the network has one million dimensions. \n",
        "- aggressive reduction to one thousand hidden dimensions $\\rightarrow$ fully connected layer with $10^6 \\times 10^3 = 10^9$ parameters. \n",
        "- one megapixel resolution may not be necessary\n",
        "  \n",
        "---"
      ],
      "id": "86281e62"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- might be able to get away with one hundred thousand pixels, our hidden layer of size 1000 grossly underestimates the number of hidden units that it takes to learn good representations of images $\\rightarrow$ practical system will still require billions of parameters.\n",
        "- learning a classifier by fitting so many parameters might require collecting an enormous dataset. \n",
        "- images exhibit rich structure that can be exploited. \n",
        "- Convolutional neural networks (CNNs) are one  way  for exploiting some of the known structure in natural images.\n",
        "\n",
        "## Invariance\n",
        "\n",
        "- want to detect an object in an image. \n",
        "- seems reasonable that whatever method we use to recognize objects should not be overly concerned with the precise location of the object in the image. \n",
        "- system should exploit this knowledge. Pigs usually do not fly and planes usually do not swim. Nonetheless, we should still recognize a pig were one to appear at the top of the image. \n",
        "- We can draw some inspiration here from the children's game \"Where's Waldo\" (depicted in :numref:`img_waldo`).\n",
        "- The game consists of a number of chaotic scenes bursting with activities."
      ],
      "id": "59cf725d"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "- Waldo shows up somewhere in each, typically lurking in some unlikely location. The reader's goal is to locate him. Despite his characteristic outfit, this can be surprisingly difficult, due to the large number of distractions.\n",
        "- However, *what Waldo looks like* does not depend upon *where Waldo is located*. We could sweep the image with a Waldo detector that could assign a score to each patch, indicating the likelihood that the patch contains Waldo. \n",
        "- In fact, many object detection and segmentation algorithms  are based on this approach :cite:`Long.Shelhamer.Darrell.2015`. \n",
        "- CNNs systematize this idea of *spatial invariance*, exploiting it to learn useful representations with fewer parameters.\n",
        "\n",
        "---"
      ],
      "id": "9f5d790d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![An image of the \"Where's Waldo\" game.](../img/where-wally-walker-books.jpg)\n",
        "\n",
        "- We can now make these intuitions more concrete by enumerating a few desiderata to guide our design of a neural network architecture suitable for computer vision:"
      ],
      "id": "af7e11dd"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "1. In the earliest layers, our network\n",
        "   should respond similarly to the same patch,\n",
        "   regardless of where it appears in the image. This principle is called *translation invariance* (or *translation equivariance*).\n",
        "2. The earliest layers of the network should focus on local regions,\n",
        "   without regard for the contents of the image in distant regions. This is the *locality* principle.\n",
        "   Eventually, these local representations can be aggregated\n",
        "   to make predictions at the whole image level.\n",
        "3. As we proceed, deeper layers should be able to capture longer-range features of the \n",
        "   image, in a way similar to higher level vision in nature. \n",
        "\n",
        "- Let's see how this translates into mathematics.\n",
        "\n",
        "\n",
        "## Constraining the MLP\n",
        "\n",
        "- To start off, we can consider an MLP with two-dimensional images $\\mathbf{X}$ as inputs and their immediate hidden representations $\\mathbf{H}$ similarly represented as matrices (they are two-dimensional tensors in code), where both $\\mathbf{X}$ and $\\mathbf{H}$ have the same shape.\n",
        "- Let that sink in. We now conceive of not only the inputs but also the hidden representations as possessing spatial structure.\n",
        "\n",
        "- Let $[\\mathbf{X}]_{i, j}$ and $[\\mathbf{H}]_{i, j}$ denote the pixel at location $(i,j)$ in the input image and hidden representation, respectively.\n",
        "  \n",
        "---"
      ],
      "id": "2ec47578"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Consequently, to have each of the hidden units receive input from each of the input pixels, we would switch from using weight matrices (as we did previously in MLPs) to representing our parameters as fourth-order weight tensors $\\mathsf{W}$.\n",
        "- Suppose that $\\mathbf{U}$ contains biases, we could formally express the fully connected layer as\n",
        "\n",
        "$$\\begin{aligned} \\left[\\mathbf{H}\\right]_{i, j} &= [\\mathbf{U}]_{i, j} + \\sum_k \\sum_l[\\mathsf{W}]_{i, j, k, l}  [\\mathbf{X}]_{k, l}\\\\ &=  [\\mathbf{U}]_{i, j} +\n",
        "\\sum_a \\sum_b [\\mathsf{V}]_{i, j, a, b}  [\\mathbf{X}]_{i+a, j+b}.\\end{aligned}$$\n",
        "\n",
        "- switch from $\\mathsf{W}$ to $\\mathsf{V}$ is entirely cosmetic for now since there is a one-to-one correspondence between coefficients in both fourth-order tensors.\n"
      ],
      "id": "ea675eb1"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "\n",
        "- We simply re-index the subscripts $(k, l)$ such that $k = i+a$ and $l = j+b$. In other words, we set $[\\mathsf{V}]_{i, j, a, b} = [\\mathsf{W}]_{i, j, i+a, j+b}$. \n",
        "- The indices $a$ and $b$ run over both positive and negative offsets, covering the entire image. For any given location ($i$, $j$) in the hidden representation $[\\mathbf{H}]_{i, j}$, we compute its value by summing over pixels in $x$, centered around $(i, j)$ and weighted by $[\\mathsf{V}]_{i, j, a, b}$. \n",
        "- Before we carry on, let's consider the total number of parameters required for a *single* layer in this parametrization: a $1000 \\times 1000$ image (1 megapixel) is mapped to a $1000 \\times 1000$ hidden representation. This requires $10^{12}$ parameters, far beyond what computers currently can handle.  \n",
        "\n",
        "## Translation Invariance\n",
        "\n",
        "- invoke the first principle established above: translation invariance :cite:`Zhang.ea.1988`.\n",
        "- This implies that a shift in the input $\\mathbf{X}$ should simply lead to a shift in the hidden representation $\\mathbf{H}$.\n",
        "- This is only possible if $\\mathsf{V}$ and $\\mathbf{U}$ do not actually depend on $(i, j)$. As such, we have $[\\mathsf{V}]_{i, j, a, b} = [\\mathbf{V}]_{a, b}$ and $\\mathbf{U}$ is a constant, say $u$. \n",
        "- As a result, we can simplify the definition for $\\mathbf{H}$:\n",
        "\n",
        "$$[\\mathbf{H}]_{i, j} = u + \\sum_a\\sum_b [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$\n",
        "\n",
        "---"
      ],
      "id": "4f7de805"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- This is a *convolution*! We are effectively weighting pixels at $(i+a, j+b)$ in the vicinity of location $(i, j)$ with coefficients $[\\mathbf{V}]_{a, b}$ to obtain the value $[\\mathbf{H}]_{i, j}$.\n",
        "- Note that $[\\mathbf{V}]_{a, b}$ needs many fewer coefficients than $[\\mathsf{V}]_{i, j, a, b}$ since it no longer depends on the location within the image. Consequently, the number of parameters required is no longer $10^{12}$ but a much more reasonable $4 \\cdot 10^6$: we still have the dependency on $a, b \\in (-1000, 1000)$. \n",
        "- In short, we have made significant progress. Time-delay neural networks (TDNNs) are some of the first examples to exploit this idea :cite:`Waibel.Hanazawa.Hinton.ea.1989`.\n",
        "\n",
        "##  Locality\n",
        "\n",
        "- Now let's invoke the second principle: locality. As motivated above, we believe that we should not have to look very far away from location $(i, j)$ in order to glean relevant information to assess what is going on at $[\\mathbf{H}]_{i, j}$. \n",
        "- This means that outside some range $|a|> \\Delta$ or $|b| > \\Delta$, we should set $[\\mathbf{V}]_{a, b} = 0$. \n",
        "- Equivalently, we can rewrite $[\\mathbf{H}]_{i, j}$ as \n",
        "$$[\\mathbf{H}]_{i, j} = u + \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} [\\mathbf{V}]_{a, b}  [\\mathbf{X}]_{i+a, j+b}.$$\n",
        ":eqlabel:`eq_conv-layer`"
      ],
      "id": "e3382c6f"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "- This reduces the number of parameters from $4 \\cdot 10^6$ to $4 \\Delta^2$, where $\\Delta$ is typically smaller than $10$. As such, we reduced the number of parameters by another 4 orders of magnitude. Note that :eqref:`eq_conv-layer`, in a nutshell, is what is called a *convolutional layer*. \n",
        "- *Convolutional neural networks* (CNNs) are a special family of neural networks that contain convolutional layers. In the deep learning research community, $\\mathbf{V}$ is referred to as a *convolution kernel*, a *filter*, or simply the layer's *weights* that are learnable parameters.\n",
        "\n",
        "---"
      ],
      "id": "aed9e8be"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- While previously, we might have required billions of parameters to represent just a single layer in an image-processing network, we now typically need just a few hundred, without altering the dimensionality of either the inputs or the hidden representations.\n",
        "- The price paid for this drastic reduction in parameters is that our features are now translation invariant and that our layer can only incorporate local information, when determining the value of each hidden activation.\n",
        "- All learning depends on imposing inductive bias. When that bias agrees with reality, we get sample-efficient models that generalize well to unseen data.\n",
        "- But of course, if those biases do not agree with reality, e.g., if images turned out not to be translation invariant, our models might struggle even to fit our training data. \n",
        "- This dramatic reduction in parameters brings us to our last desideratum,  namely that deeper layers should represent larger and more complex aspects of an image. This can be achieved by interleaving nonlinearities and convolutional layers repeatedly. \n",
        "\n",
        "## Convolutions\n",
        "\n",
        "- Let's briefly review why :eqref:`eq_conv-layer` is called a convolution. \n",
        "- In mathematics, the *convolution* between two functions :cite:`Rudin.1973`, say $f, g: \\mathbb{R}^d \\to \\mathbb{R}$ is defined as\n",
        "\n",
        "$$(f * g)(\\mathbf{x}) = \\int f(\\mathbf{z}) g(\\mathbf{x}-\\mathbf{z}) d\\mathbf{z}.$$\n",
        "\n",
        "- That is, we measure the overlap between $f$ and $g$ when one function is \"flipped\" and shifted by $\\mathbf{x}$. \n",
        "- Whenever we have discrete objects, the integral turns into a sum. For instance, for vectors from the set of square summable infinite dimensional vectors with index running over $\\mathbb{Z}$ we obtain the following definition:"
      ],
      "id": "b9d0e843"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "$$(f * g)(i) = \\sum_a f(a) g(i-a).$$\n",
        "\n",
        "- For two-dimensional tensors, we have a corresponding sum with indices $(a, b)$ for $f$ and $(i-a, j-b)$ for $g$, respectively: \n",
        "- \n",
        "$$(f * g)(i, j) = \\sum_a\\sum_b f(a, b) g(i-a, j-b).$$\n",
        ":eqlabel:`eq_2d-conv-discrete`\n",
        "\n",
        "---"
      ],
      "id": "2d8281a2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- This looks similar to :eqref:`eq_conv-layer`, with one major difference. \n",
        "- Rather than using $(i+a, j+b)$, we are using the difference instead. Note, though, that this distinction is mostly cosmetic since we can always match the notation between :eqref:`eq_conv-layer` and :eqref:`eq_2d-conv-discrete`.\n",
        "- Our original definition in :eqref:`eq_conv-layer` more properly describes a *cross-correlation*. We will come back to this in the following section.\n",
        "\n",
        "## Channels\n",
        "\n",
        "- Returning to Waldo detector, let's see what this looks like. \n",
        "- convolutional layer picks windows of a given size and weighs intensities according to the filter $\\mathsf{V}$, as demonstrated in :numref:`fig_waldo_mask`. \n",
        "- aim to learn a model so that wherever the \"waldoness\" is highest, we should find a peak in the hidden layer representations. \n",
        "\n",
        "![Detect Waldo.](../img/waldo-mask.jpg){width=400px}"
      ],
      "id": "46e23906"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "- There is just one problem with this approach. So far, we blissfully ignored that images consist of 3 channels: red, green, and blue.  \n",
        "- In sum, images are not two-dimensional objects but rather third-order tensors, characterized by a height, width, and channel, e.g., with shape $1024 \\times 1024 \\times 3$ pixels.  \n",
        "- While the first two of these axes concern spatial relationships, the third can be regarded as assigning a multidimensional representation to each pixel location. \n",
        "- We thus index $\\mathsf{X}$ as $[\\mathsf{X}]_{i, j, k}$. The convolutional filter has to adapt accordingly. Instead of $[\\mathbf{V}]_{a,b}$, we now have $[\\mathsf{V}]_{a,b,c}$.\n",
        "\n",
        "---"
      ],
      "id": "f230a72a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Moreover, just as our input consists of a third-order tensor, it turns out to be a good idea to similarly formulate our hidden representations as third-order tensors $\\mathsf{H}$.\n",
        "- In other words, rather than just having a single hidden representation corresponding to each spatial location, we want an entire vector of hidden representations corresponding to each spatial location.\n",
        "- We could think of the hidden representations as comprising a number of two-dimensional grids stacked on top of each other. As in the inputs, these are sometimes called *channels*.\n",
        "- They are also sometimes called *feature maps*, as each provides a spatialized set of learned features to the subsequent layer. Intuitively, you might imagine that at lower layers that are closer to inputs, some channels could become specialized to recognize edges while others could recognize textures."
      ],
      "id": "c375e0b2"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "- To support multiple channels in both inputs ($\\mathsf{X}$) and hidden representations ($\\mathsf{H}$), we can add a fourth coordinate to $\\mathsf{V}$: $[\\mathsf{V}]_{a, b, c, d}$. \n",
        "- Putting everything together we have:\n",
        "\n",
        "$$[\\mathsf{H}]_{i,j,d} = \\sum_{a = -\\Delta}^{\\Delta} \\sum_{b = -\\Delta}^{\\Delta} \\sum_c [\\mathsf{V}]_{a, b, c, d} [\\mathsf{X}]_{i+a, j+b, c},$$\n",
        ":eqlabel:`eq_conv-layer-channels`\n",
        "\n",
        "- where $d$ indexes the output channels in the hidden representations $\\mathsf{H}$. The subsequent convolutional layer will go on to take a third-order tensor, $\\mathsf{H}$, as input. \n",
        "- Being more general, :eqref:`eq_conv-layer-channels` is the definition of a convolutional layer for multiple channels, where $\\mathsf{V}$ is a kernel or filter of the layer.\n",
        "\n",
        "---"
      ],
      "id": "4ccf553f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- still many operations that we need to address. \n",
        "- For instance, we need to figure out how to combine all the hidden representations to a single output, e.g., whether there is a Waldo *anywhere* in the image. \n",
        "- We also need to decide how to compute things efficiently, how to combine multiple layers, appropriate activation functions, and how to make reasonable design choices to yield networks that are effective in practice. We turn to these issues in the remainder of the chapter.\n",
        "\n",
        "## Summary and Discussion\n",
        "\n",
        "- derived the structure of convolutional neural networks from first principles. \n",
        "- -unclear whether this is what led to the invention of CNNs, it is satisfying to know that they are the *right* choice when applying reasonable principles to how image processing and computer vision algorithms should operate, at least at lower levels. \n",
        "- In particular, translation invariance in images implies that all patches of an image will be treated in the same manner. \n",
        "- Locality means that only a small neighborhood of pixels will be used to compute the corresponding hidden representations. Some of the earliest references to CNNs are in the form of the Neocognitron :cite:`Fukushima.1982`. \n",
        "- second principle that we encountered in our reasoning is how to reduce the number of parameters in a function class without limiting its expressive power, at least, whenever certain assumptions on the model hold. \n",
        "- saw a dramatic reduction of complexity as a result of this restriction, turning computationally and statistically infeasible problems into tractable models. "
      ],
      "id": "35ba2904"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "- Adding channels allowed us to bring back some of the complexity that was lost due to the restrictions imposed on the convolutional kernel by locality and translation invariance. Note that channels are quite a natural addition beyond red, green, and blue. \n",
        "- Many images have tens to hundreds of channels, generating hyperspectral images instead. They report data on many different wavelengths. In the following we will see how to use convolutions effectively to manipulate the dimensionality of the images they operate on, how to move from location-based to channel-based representations and how to deal with large numbers of categories efficiently. \n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Assume that the size of the convolution kernel is $\\Delta = 0$.\n",
        "   Show that in this case the convolution kernel\n",
        "   implements an MLP independently for each set of channels. This leads to the Network in Network \n",
        "   architectures :cite:`Lin.Chen.Yan.2013`. \n",
        "1. Audio data is often represented as a one-dimensional sequence. \n",
        "    1. When might you want to impose locality and translation invariance for audio? \n",
        "    1. Derive the convolution operations for audio.\n",
        "    1. Can you treat audio using the same tools as computer vision? Hint: use the spectrogram.\n",
        "1. Why might translation invariance not be a good idea after all? Give an example. \n",
        "1. Do you think that convolutional layers might also be applicable for text data?\n",
        "   Which problems might you encounter with language?\n",
        "1. What happens with convolutions when an object is at the boundary of an image. \n",
        "1. Prove that the convolution is symmetric, i.e., $f * g = g * f$.\n",
        "1. Prove the convolution theorem, i.e., $f * g = \\mathcal{F}^{-1}\\left[\\mathcal{F}[f] \\cdot \\mathcal{F}[g]\\right]$. \n",
        "   Can you use it to accelerate convolutions? \n",
        "\n",
        "## Convolutions for Images\n",
        "\n",
        "- understand how convolutional layers work in theory, we are ready to see how they work in practice. \n",
        "- Building on our motivation of convolutional neural networks as efficient architectures for exploring structure in image data, we stick with images as our running example. \n"
      ],
      "id": "1be77686"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from d2l import torch as d2l\n",
        "import torch\n",
        "from torch import nn"
      ],
      "id": "71d3c46f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## The Cross-Correlation Operation\n",
        "\n",
        "- Recall that strictly speaking, convolutional layers are a  misnomer, since the operations they express are more accurately described as cross-correlations.\n",
        "- Based on our descriptions of convolutional layers in :numref:`sec_why-conv`, in such a layer, an input tensor and a kernel tensor are combined to produce an output tensor through a (**cross-correlation operation.**)\n",
        "\n",
        "- ignore channels for now and see how this works with two-dimensional data and hidden representations. \n",
        "- In :numref:`fig_correlation`, the input is a two-dimensional tensor with a height of 3 and width of 3. We mark the shape of the tensor as $3 \\times 3$ or ($3$, $3$). The height and width of the kernel are both 2. The shape of the *kernel window* (or *convolution window*) is given by the height and width of the kernel (here it is $2 \\times 2$).\n",
        "\n",
        "---"
      ],
      "id": "982c62e7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Two-dimensional cross-correlation operation. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\times0+1\\times1+3\\times2+4\\times3=19$.](../img/correlation.svg)\n",
        "\n",
        "- In the two-dimensional cross-correlation operation, we begin with the convolution window positioned at the upper-left corner of the input tensor and slide it across the input tensor, both from left to right and top to bottom. \n",
        "- When the convolution window slides to a certain position, the input subtensor contained in that window and the kernel tensor are multiplied elementwise and the resulting tensor is summed up yielding a single scalar value. \n"
      ],
      "id": "c56cd93a"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "\n",
        "- This result gives the value of the output tensor at the corresponding location. Here, the output tensor has a height of 2 and width of 2 and the four elements are derived from the two-dimensional cross-correlation operation:\n",
        "\n",
        "$$\n",
        "0\\times0+1\\times1+3\\times2+4\\times3=19,\\\\\n",
        "1\\times0+2\\times1+4\\times2+5\\times3=25,\\\\\n",
        "3\\times0+4\\times1+6\\times2+7\\times3=37,\\\\\n",
        "4\\times0+5\\times1+7\\times2+8\\times3=43.\n",
        "$$\n",
        "\n",
        "- along each axis, the output size is slightly smaller than the input size. \n",
        "- Because the kernel has width and height greater than one, we can only properly compute the cross-correlation for locations where the kernel fits wholly within the image, the output size is given by the input size $n_h \\times n_w$ minus the size of the convolution kernel $k_h \\times k_w$ via\n",
        "\n",
        "$$(n_h-k_h+1) \\times (n_w-k_w+1).$$\n",
        "\n",
        "---"
      ],
      "id": "312e567d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- This is the case since we need enough space to \"shift\" the convolution kernel across the image. Later we will see how to keep the size unchanged by padding the image with zeros around its boundary so that there is enough space to shift the kernel. Next, we implement this process in the `corr2d` function, which accepts an input tensor `X` and a kernel tensor `K` and returns an output tensor `Y`. \n"
      ],
      "id": "71cce5e1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def corr2d(X, K):  #@save\n",
        "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
        "    h, w = K.shape\n",
        "    Y = d2l.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
        "    for i in range(Y.shape[0]):\n",
        "        for j in range(Y.shape[1]):\n",
        "            Y[i, j] = d2l.reduce_sum((X[i: i + h, j: j + w] * K))\n",
        "    return Y"
      ],
      "id": "3d11058f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "- We can construct the input tensor `X` and the kernel tensor `K` from :numref:`fig_correlation` to [**validate the output of the above implementation**] of the two-dimensional cross-correlation operation.\n"
      ],
      "id": "3fe06759"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = d2l.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
        "K = d2l.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
        "corr2d(X, K)"
      ],
      "id": "1eb97fbe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## Convolutional Layers\n",
        "\n",
        "- convolutional layer cross-correlates the input and kernel and adds a scalar bias to produce an output. \n",
        "- The two parameters of a convolutional layer are the kernel and the scalar bias. \n",
        "- When training models based on convolutional layers, we typically initialize the kernels randomly, just as we would with a fully connected layer.\n",
        "\n",
        "- We are now ready to [**implement a two-dimensional convolutional layer**] based on the `corr2d` function defined above. \n",
        "- In the `__init__` constructor method, we declare `weight` and `bias` as the two model parameters. The forward propagation method calls the `corr2d` function and adds the bias.\n",
        "\n",
        "---"
      ],
      "id": "367eb3cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class Conv2D(nn.Module):\n",
        "    def __init__(self, kernel_size):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return corr2d(x, self.weight) + self.bias"
      ],
      "id": "0839468b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- In $h \\times w$ convolution or a $h \\times w$ convolution kernel, the height and width of the convolution kernel are $h$ and $w$, respectively.\n",
        "- We also refer to a convolutional layer with a $h \\times w$ convolution kernel simply as a $h \\times w$ convolutional layer.\n",
        "\n",
        "## Object Edge Detection in Images\n",
        "\n",
        "- take a moment to parse [**a simple application of a convolutional layer: detecting the edge of an object in an image**] by finding the location of the pixel change.\n",
        "- First, we construct an \"image\" of $6\\times 8$ pixels. The middle four columns are black (0) and the rest are white (1).\n"
      ],
      "id": "10595d5a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = d2l.ones((6, 8))\n",
        "X[:, 2:6] = 0\n",
        "X"
      ],
      "id": "3a71abdd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- next construct a kernel `K` with a height of 1 and a width of 2. \n",
        "- perform the cross-correlation operation with the input, if the horizontally adjacent elements are the same, the output is 0. Otherwise, the output is non-zero.\n",
        "- kernel is special case of a finite difference operator. At location $(i,j)$ it computes $x_{i,j} - x_{(i+1),j}$, i.e., it computes the difference between the values of horizontally adjacent pixels. \n",
        "- discrete approximation of the first derivative in the horizontal direction. After all, for a function $f(i,j)$ its derivative $-\\partial_i f(i,j) = \\lim_{\\epsilon \\to 0} \\frac{f(i,j) - f(i+\\epsilon,j)}{\\epsilon}$. Let's see how this works in practice."
      ],
      "id": "ce6d52ff"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ],
      "id": "cc9e4322"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "K = d2l.tensor([[1.0, -1.0]])"
      ],
      "id": "5a810504",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-  ready to perform the cross-correlation operation with arguments `X` (our input) and `K` (our kernel). As you can see, [**we detect 1 for the edge from white to black and -1 for the edge from black to white.**] All other outputs take value 0.\n"
      ],
      "id": "ba5f135d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Y = corr2d(X, K)\n",
        "Y"
      ],
      "id": "690963c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We can now apply the kernel to the transposed image. As expected, it vanishes. [**The kernel `K` only detects vertical edges.**]\n"
      ],
      "id": "aee096e3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "corr2d(d2l.transpose(X), K)"
      ],
      "id": "1888a0d6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## Learning a Kernel\n",
        "\n",
        "- Designing an edge detector by finite differences `[1, -1]` is neat if we know this is precisely what we are looking for. However, as we look at larger kernels, and consider successive layers of convolutions, it might be impossible to specify precisely what each filter should be doing manually.\n",
        "- Now let's see whether we can [**learn the kernel that generated `Y` from `X`**] by looking at the input--output pairs only. \n",
        "- We first construct a convolutional layer and initialize its kernel as a random tensor. Next, in each iteration, we will use the squared error to compare `Y` with the output of the convolutional layer.\n",
        "- We can then calculate the gradient to update the kernel. For the sake of simplicity, in the following we use the built-in class for two-dimensional convolutional layers and ignore the bias.\n",
        "\n",
        "---"
      ],
      "id": "89bc844c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## Construct a two-dimensional convolutional layer with 1 output channel and a\n",
        "## kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\n",
        "conv2d = nn.LazyConv2d(1, kernel_size=(1, 2), bias=False)\n",
        "\n",
        "## The two-dimensional convolutional layer uses four-dimensional input and\n",
        "## output in the format of (example, channel, height, width), where the batch\n",
        "## size (number of examples in the batch) and the number of channels are both 1\n",
        "X = X.reshape((1, 1, 6, 8))\n",
        "Y = Y.reshape((1, 1, 6, 7))\n",
        "lr = 3e-2  ## Learning rate\n",
        "\n",
        "for i in range(10):\n",
        "    Y_hat = conv2d(X)\n",
        "    l = (Y_hat - Y) ** 2\n",
        "    conv2d.zero_grad()\n",
        "    l.sum().backward()\n",
        "    ## Update the kernel\n",
        "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
        "    if (i + 1) % 2 == 0:\n",
        "        print(f'epoch {i + 1}, loss {l.sum():.3f}')"
      ],
      "id": "e091dd0e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Note that the error has dropped to a small value after 10 iterations. Now we will **take a look at the kernel tensor we learned.**"
      ],
      "id": "6545be81"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ],
      "id": "d4e7da31"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "d2l.reshape(conv2d.weight.data, (1, 2))"
      ],
      "id": "7777e2db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "- Indeed, the learned kernel tensor is remarkably close to the kernel tensor `K` we defined earlier.\n",
        "\n",
        "## Cross-Correlation and Convolution\n",
        "\n",
        "- Recall our observation from :numref:`sec_why-conv` of the correspondence between the cross-correlation and convolution operations. \n",
        "- consider two-dimensional convolutional layers. What if such layers perform strict convolution operations as defined in :eqref:`eq_2d-conv-discrete` instead of cross-correlations?\n",
        "- for strict *convolution* operation, we only need to flip the two-dimensional kernel tensor both horizontally and vertically, and then perform the *cross-correlation* operation with the input tensor.\n",
        "- kernels are learned from data in deep learning --> outputs of convolutional layers remain unaffected no matter such layers perform either the strict convolution operations or the cross-correlation operations.\n",
        "\n",
        "---"
      ],
      "id": "7ae4059a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- suppose that a convolutional layer performs *cross-correlation* and learns the kernel in :numref:`fig_correlation`, which is denoted as the matrix $\\mathbf{K}$ here. \n",
        "- Assuming that other conditions remain unchanged, when this layer performs strict *convolution* instead, the learned kernel $\\mathbf{K}'$ will be the same as $\\mathbf{K}$ after $\\mathbf{K}'$ is flipped both horizontally and vertically.\n",
        "- That is to say, when the convolutional layer performs strict *convolution* for the input in :numref:`fig_correlation` and $\\mathbf{K}'$, the same output in :numref:`fig_correlation` (cross-correlation of the input and $\\mathbf{K}$) will be obtained.\n",
        "- continue to refer to the cross-correlation operation as a convolution even though, strictly-speaking, it is slightly different.\n",
        "- Besides, we use the term *element* to refer to an entry (or component) of any tensor representing a layer representation or a convolution kernel.\n",
        "\n",
        "## Feature Map and Receptive Field\n",
        "\n",
        "- As described in *Why Con Channels*, the convolutional layer output in :numref:`fig_correlation` is sometimes called a *feature map*, as it can be regarded as the learned representations (features) in the spatial dimensions (e.g., width and height) to the subsequent layer. \n",
        "- in CNNs, for any element $x$ of some layer, its *receptive field* refers to all the elements (from all the previous layers) that may affect the calculation of $x$ during the forward propagation.\n",
        "- receptive field may be larger than the actual size of the input.\n",
        "- continue to use :numref:`fig_correlation` to explain the receptive field. Given the $2 \\times 2$ convolution kernel, the receptive field of the shaded output element (of value $19$) is the four elements in the shaded portion of the input.\n",
        "- denote the $2 \\times 2$ output as $\\mathbf{Y}$ and consider a deeper CNN with an additional $2 \\times 2$ convolutional layer that takes $\\mathbf{Y}$ as its input, outputting a single element $z$.\n",
        "- In this case, the receptive field of $z$ on $\\mathbf{Y}$ includes all the four elements of $\\mathbf{Y}$, while the receptive field on the input includes all the nine input elements.\n",
        "- Thus, when any element in a feature map needs a larger receptive field to detect input features over a broader area, we can build a deeper network."
      ],
      "id": "2011a4ca"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "--- \n",
        "\n",
        "- Receptive fields derive their name from neurophysiology. In a series of experiments :cite:`Hubel.Wiesel.1959,Hubel.Wiesel.1962,Hubel.Wiesel.1968` on a range of animals and different stimuli, Hubel and  iesel explored the response of what is called the visual cortex on said stimuli. By and large they found that lower levels respond to edges and related shapes. Later on, :citet:`Field.1987` illustrated this effect on natural images with, what can only be called, convolutional kernels. \n",
        "\n",
        "---"
      ],
      "id": "4de3b2dd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We reprint a key figure in :numref:`field_visual` to illustrate the striking similarities.\n",
        "\n",
        "![Figure and caption taken from :citet:`Field.1987`: An example of coding with six different channels. (Left) Examples of the six types of sensor associated with each channel. (Right) Convolution of the image in (Middle) with the six sensors shown in (Left). The response of the individual sensors is determined by sampling these filtered images at a distance proportional to the size of the sensor (shown with dots). This diagram shows the response of only the even symmetric sensors.](../img/field-visual.png){width=600px}\n"
      ],
      "id": "b9698b64"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "- As it turns out, this relation even holds for the features computed by deeper layers of networks trained on image classification tasks, as demonstrated e.g., in :citet:`Kuzovkin.Vicente.Petton.ea.2018`. Suffice it to say, convolutions have proven to be an incredibly powerful tool for computer vision, both in biology and in code. As such, it is not surprising (in hindsight) that they heralded the recent success in deep learning.\n",
        "\n",
        "## Summary\n",
        "\n",
        "- core computation required for a convolutional layer is a cross-correlation operation. We saw that a simple nested for-loop is all that is required to compute its value. \n",
        "- If we have multiple input and multiple output channels, we are  performing a matrix-matrix operation between channels. \n",
        "- As can be seen, the computation is straightforward and, most importantly, highly *local*. \n",
        "- This affords significant hardware optimization and many recent results in computer vision are only possible due to that. After all, it means that chip designers can invest into fast computation rather than memory, when it comes to optimizing for convolutions. While this may not lead to optimal designs for other applications, it opens the door to ubiquitous and affordable computer vision.\n",
        "- convolutions can be used for many purposes such as to detect edges and lines, to blur images, or to sharpen them. \n",
        "- Most importantly, it is not necessary that the statistician (or engineer) invents suitable filters. \n",
        "- Instead, we can simply *learn* them from data. This replaces feature engineering heuristics by evidence-based statistics. \n",
        "- filters are not just advantageous for building deep networks but also correspond to receptive fields and feature maps in the brain\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Construct an image `X` with diagonal edges.\n",
        "    1. What happens if you apply the kernel `K` in this section to it?\n",
        "    1. What happens if you transpose `X`?\n",
        "    1. What happens if you transpose `K`?\n",
        "1. Design some kernels manually.\n",
        "    1. Given a directional vector $\\mathbf{v} = (v_1, v_2)$, derive an edge-detection kernel that detects\n",
        "       edges orthogonal to $\\mathbf{v}$, i.e., edges in the direction $(v_2, -v_1)$.\n",
        "    1. Derive a finite difference operator for the second derivative. What is the minimum\n",
        "       size of the convolutional kernel associate with it? Which structures in images respond most strongly to it?\n",
        "    1. How would you design a blur kernel? Why might you want to use such a kernel?\n",
        "    1. What is the minimum size of a kernel to obtain a derivative of order $d$?\n",
        "1. When you try to automatically find the gradient for the `Conv2D` class we created, what kind of error message do you see?\n",
        "1. How do you represent a cross-correlation operation as a matrix multiplication by changing the input and kernel tensors?\n",
        "\n",
        "\n",
        "## Padding and Stride\n",
        "\n",
        "- Recall the example of a convolution in :numref:`fig_correlation`.  The input had both a height and width of 3 and the convolution kernel had both a height and width of 2, yielding an output representation with dimension $2\\times2$. \n",
        "- Assuming that the input shape is $n_h\\times n_w$ and the convolution kernel shape is $k_h\\times k_w$, the output shape will be $(n_h-k_h+1) \\times (n_w-k_w+1)$:  \n",
        "- we can only shift the convolution kernel so far until it runs out of pixels to apply the convolution to. \n",
        "- In the following we will explore a number of techniques,  including padding and strided convolutions, that offer more control over the size of the output.  \n",
        "- As motivation, note that since kernels generally have width and height greater than $1$, after applying many successive convolutions, we tend to wind up with outputs that are considerably smaller than our input. If we start with a $240 \\times 240$ pixel image, $10$ layers of $5 \\times 5$ convolutions reduce the image to $200 \\times 200$ pixels, slicing off $30 \\%$ of the image and with it obliterating any interesting information on the boundaries of the original image. \n",
        "\n",
        "---"
      ],
      "id": "88e98ce2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- *Padding* most popular tool for handling this issue. \n",
        "- may want to reduce the dimensionality drastically, e.g., if we find the original input resolution to be unwieldy. *Strided convolutions* are a popular technique that can help in these instances.\n"
      ],
      "id": "a87513a2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "id": "d31a8bc5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Padding\n",
        "\n",
        "- As described above, one tricky issue when applying convolutional layers is that we tend to lose pixels on the perimeter of our image. Consider :numref:`img_conv_reuse` that depicts the pixel utilization as a function of the convolution kernel size and the position within the image. The pixels in the corners are hardly used at all. \n",
        "\n",
        "![Pixel utilization for convolutions of size $1 \\times 1$, $2 \\times 2$, and $3 \\times 3$ respectively.](../img/conv-reuse.svg)\n"
      ],
      "id": "d6c3b14b"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "- typically use small kernels, for any given convolution, we might only lose a few pixels, but this can add up as we apply many successive convolutional layers. \n",
        "- one straightforward solution to this problem is to add extra pixels of filler around the boundary of our input image, thus increasing the effective size of the image.\n",
        "- Typically, we set the values of the extra pixels to zero. In :numref:`img_conv_pad`, we pad a $3 \\times 3$ input, increasing its size to $5 \\times 5$.\n",
        "- The corresponding output then increases to a $4 \\times 4$ matrix. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\times0+0\\times1+0\\times2+0\\times3=0$.\n",
        "\n",
        "---"
      ],
      "id": "816395e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Two-dimensional cross-correlation with padding.](../img/conv-pad.svg){width=500px}\n",
        "\n",
        "\n",
        "- In general, if we add a total of $p_h$ rows of padding (roughly half on top and half on bottom) and a total of $p_w$ columns of padding (roughly half on the left and half on the right), the output shape will be\n",
        "\n",
        "$$(n_h-k_h+p_h+1)\\times(n_w-k_w+p_w+1).$$"
      ],
      "id": "29eb5d98"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "- This means that the height and width of the output will increase by $p_h$ and $p_w$, respectively.\n",
        "\n",
        "- In many cases, we will want to set $p_h=k_h-1$ and $p_w=k_w-1$ to give the input and output the same height and width. This will make it easier to predict the output shape of each layer when constructing the network. Assuming that $k_h$ is odd here, we will pad $p_h/2$ rows on both sides of the height. If $k_h$ is even, one possibility is to pad $\\lceil p_h/2\\rceil$ rows on the top of the input and $\\lfloor p_h/2\\rfloor$ rows on the bottom. We will pad both sides of the width in the same way.\n",
        "- CNNs commonly use convolution kernels with odd height and width values, such as 1, 3, 5, or 7. Choosing odd kernel sizes has the benefit that we can preserve the dimensionality while padding with the same number of rows on top and bottom, and the same number of columns on left and right.\n",
        "- practice of using odd kernels and padding to precisely preserve dimensionality offers a clerical benefit. \n",
        "- any two-dimensional tensor `X`, when the kernel's size is odd and the number of adding rows and columns on all sides are the same, producing an output with the same height and width as the input, we know that the output `Y[i, j]` is calculated by cross-correlation of the input and convolution kernel with the window centered on `X[i, j]`.\n",
        "\n",
        "---"
      ],
      "id": "fc641a26"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- In the following example, we create a two-dimensional convolutional layer with a height and width of 3 and (**apply 1 pixel of padding on all sides.**) \n",
        "- input with a height and width of 8, we find that the height and width of the output is also 8.\n"
      ],
      "id": "71c6e58e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## We define a helper function to calculate convolutions. It initializes the\n",
        "## convolutional layer weights and performs corresponding dimensionality\n",
        "## elevations and reductions on the input and output\n",
        "def comp_conv2d(conv2d, X):\n",
        "    ## (1, 1) indicates that batch size and the number of channels are both 1\n",
        "    X = X.reshape((1, 1) + X.shape)\n",
        "    Y = conv2d(X)\n",
        "    ## Strip the first two dimensions: examples and channels\n",
        "    return Y.reshape(Y.shape[2:])\n",
        "\n",
        "## 1 row and column is padded on either side, so a total of 2 rows or columns\n",
        "## are added\n",
        "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1)\n",
        "X = torch.rand(size=(8, 8))\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "id": "f88797ed",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "-  when the height and width of the convolution kernel are different, we can make the output and input have the same height and width by [**setting different padding numbers for height and width.**]\n"
      ],
      "id": "dbd48b23"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "## We use a convolution kernel with height 5 and width 3. The padding on either\n",
        "## side of the height and width are 2 and 1, respectively\n",
        "conv2d = nn.LazyConv2d(1, kernel_size=(5, 3), padding=(2, 1))\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "id": "76aceda1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Stride\n",
        "\n",
        "- computing the cross-correlation, we start with the convolution window at the upper-left corner of the input tensor,  then slide it over all locations both down and to the right. \n",
        "- previous: defaulted to sliding one element at a time. \n",
        "- sometimes, either for computational efficiency or because we wish to downsample, we move our window more than one element at a time, skipping the intermediate locations. \n",
        "- particularly useful if the convolution  kernel is large since it captures a large area of the underlying image.\n",
        "- refer to the number of rows and columns traversed per slide as *stride*. \n",
        "- So far, we have used strides of 1, both for height and width. Sometimes, we may want to use a larger stride. :numref:`img_conv_stride` shows a two-dimensional cross-correlation operation with a stride of 3 vertically and 2 horizontally. "
      ],
      "id": "6dc8433f"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "- The shaded portions are the output elements as well as the input and kernel tensor elements used for the output computation: $0\\times0+0\\times1+1\\times2+2\\times3=8$, $0\\times0+6\\times1+0\\times2+0\\times3=6$.\n",
        "- We can see that when the second element of the first column is generated, the convolution window slides down three rows. \n",
        "- The convolution window slides two columns to the right when the second element of the first row is generated. \n",
        "- When the convolution window continues to slide two columns to the right on the input, there is no output because the input element cannot fill the window (unless we add another column of padding).\n",
        "\n",
        "---"
      ],
      "id": "1f64be0a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Cross-correlation with strides of 3 and 2 for height and width, respectively.](../img/conv-stride.svg){width=300px}\n",
        "\n",
        "\n",
        "- In general, when the stride for the height is $s_h$ and the stride for the width is $s_w$, the output shape is\n",
        "\n",
        "$$\\lfloor(n_h-k_h+p_h+s_h)/s_h\\rfloor \\times \\lfloor(n_w-k_w+p_w+s_w)/s_w\\rfloor.$$\n",
        "\n",
        "- If we set $p_h=k_h-1$ and $p_w=k_w-1$, then the output shape can be simplified to $\\lfloor(n_h+s_h-1)/s_h\\rfloor \\times \\lfloor(n_w+s_w-1)/s_w\\rfloor$. \n",
        "- Going a step further, if the input height and width are divisible by the strides on the height and width, then the output shape will be $(n_h/s_h) \\times (n_w/s_w)$. Below, we [**set the strides on both the height and width to 2**], thus halving the input height and width. "
      ],
      "id": "9e68bd58"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n"
      ],
      "id": "ccc80e6f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1, stride=2)\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "id": "2c854030",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Let's look at (**a slightly more complicated example**).\n"
      ],
      "id": "92692b13"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "conv2d = nn.LazyConv2d(1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
        "comp_conv2d(conv2d, X).shape"
      ],
      "id": "542e6fe4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## Summary and Discussion\n",
        "\n",
        "- Padding can increase the height and width of the output. This is often used to give the output the same height and width as the input to avoid undesirable shrinkage of the output. \n",
        "- Moreover, it ensures that all pixels are used equally frequently. \n",
        "- Typically we pick symmetric padding on both sides of the input height and width. In this case we refer to $(p_h, p_w)$ padding. Most commonly we set $p_h = p_w$, in which case we simply state that we choose padding $p$. \n",
        "- A similar convention applies to strides. When horizontal stride $s_h$ and vertical stride $s_w$ match, we simply talk about stride $s$. \n",
        "- The stride can reduce the resolution of the output, for example reducing the height and width of the output to only $1/n$ of the height and width of the input for $n > 1$. By default, the padding is 0 and the stride is 1. \n",
        "- So far all padding that we discussed simply extended images with zeros. \n",
        "\n",
        "---"
      ],
      "id": "ba0a68d1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- This has significant computational benefit since it is trivial to accomplish. \n",
        "- Moreover, operators can be engineered to take advantage of this padding implicitly without the need to allocate additional memory. \n",
        "- At the same time, it allows CNNs to encode implicit position information within an image, simply by learning where the \"whitespace\" is. \n",
        "- There are many alternatives to zero-padding. :citet:`Alsallakh.Kokhlikyan.Miglani.ea.2020` provided an extensive overview of alternatives (albeit without a clear case to use nonzero paddings unless artifacts occur). \n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Given the last code example in this section with kernel size $(3, 5)$, padding $(0, 1)$, and stride $(3, 4)$, \n",
        "   calculate the output shape to check if it is consistent with the experimental result.\n",
        "1. For audio signals, what does a stride of 2 correspond to?\n",
        "1. Implement mirror padding, i.e., padding where the border values are simply mirrored to extend tensors. \n",
        "1. What are the computational benefits of a stride larger than 1?\n",
        "1. What might be statistical benefits of a stride larger than 1?\n",
        "1. How would you implement a stride of $\\frac{1}{2}$? What does it correspond to? When would this be useful?\n",
        "\n",
        "## Multiple Input and Multiple Output Channels\n",
        "- while we described the multiple channels that comprise each image (e.g., color images have the standard RGB channels to indicate the amount of red, green and blue) and convolutional layers for multiple channels in :numref:`subsec_why-conv-channels`, until now, we simplified all of our numerical examples by working with just a single input and a single output channel. \n",
        "- This allowed us to think of our inputs, convolution kernels, and outputs each as two-dimensional tensors.\n",
        "- When we add channels into the mix, our inputs and hidden representations both become three-dimensional tensors. For example, each RGB input image has shape $3\\times h\\times w$. We refer to this axis, with a size of 3, as the *channel* dimension. The notion of channels is as old as CNNs themselves. For instance LeNet5 :cite:`LeCun.Jackel.Bottou.ea.1995` uses them.  In this section, we will take a deeper look at convolution kernels with multiple input and multiple output channels.\n"
      ],
      "id": "5725db78"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from d2l import torch as d2l\n",
        "import torch"
      ],
      "id": "9e267a78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiple Input Channels\n",
        "\n",
        "- When the input data contains multiple channels, we need to construct a convolution kernel with the same number of input channels as the input data, so that it can perform cross-correlation with the input data.\n",
        "- Assuming that the number of channels for the input data is $c_i$, the number of input channels of the convolution kernel also needs to be $c_i$. If our convolution kernel's window shape is $k_h\\times k_w$, then when $c_i=1$, we can think of our convolution kernel as just a two-dimensional tensor of shape $k_h\\times k_w$.\n",
        "- However, when $c_i>1$, we need a kernel that contains a tensor of shape $k_h\\times k_w$ for *every* input channel. Concatenating these $c_i$ tensors together yields a convolution kernel of shape $c_i\\times  _h\\times k_w$. \n",
        "- Since the input and convolution kernel each have $c_i$ channels, we can perform a cross-correlation operation on the two-dimensional tensor of the input and the two-dimensional tensor of the convolution  kernel for each channel, adding the $c_i$ results together (summing over the channels) to yield a two-dimensional tensor.\n",
        "- This is the result of a two-dimensional cross-correlation between a multi-channel input and a multi-input-channel convolution kernel. "
      ],
      "id": "bbf96d93"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "- :numref:`fig_conv_multi_in` provides an example  of a two-dimensional cross-correlation with two input channels. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: \n",
        "- $(1\\times1+2\\times2+4\\times3+5\\times4)+(0\\times0+1\\times1+3\\times2+4\\times3)=56$.\n",
        "\n",
        "![Cross-correlation computation with 2 input channels.](../img/conv-multi-in.svg)\n",
        ":label:`fig_conv_multi_in`\n",
        "\n",
        "- To make sure we really understand what is going on here, we can (**implement cross-correlation operations with multiple input channels**) ourselves. \n",
        "- Notice that all we are doing is performing a cross-correlation operation per channel and then adding up the results.\n"
      ],
      "id": "e0ed15b7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def corr2d_multi_in(X, K):\n",
        "    ## Iterate through the 0th dimension (channel) of K first, then add them up\n",
        "    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))"
      ],
      "id": "e19def48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---"
      ],
      "id": "d057c3b4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- can construct the input tensor `X` and the kernel tensor `K` corresponding to the values in :numref:`fig_conv_multi_in` to (**validate the output**) of the cross-correlation operation.\n"
      ],
      "id": "939a8114"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = d2l.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
        "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
        "K = d2l.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
        "\n",
        "corr2d_multi_in(X, K)"
      ],
      "id": "6a9c109f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiple Output Channels\n",
        "- Regardless of the number of input channels, so far we always ended up with one output channel. However, as we discussed in :numref:`subsec_why-conv-channels`, it turns out to be essential to have multiple channels at each layer. In the most popular neural network architectures, we actually increase the channel dimension as we go deeper in the neural network, typically downsampling to trade off spatial resolution for greater *channel depth*. \n",
        "- Intuitively, could think of each channel as responding to a different set of features. \n",
        "- reality is a bit more complicated than this. naive interpretation would suggest that representations are learned independently per pixel or per channel. \n",
        "- Instead, channels are optimized to be jointly useful. This means that rather than mapping a single channel to an edge detector, it may simply mean  that some direction in channel space corresponds to detecting edges.\n",
        "- Denote by $c_i$ and $c_o$ the number of input and output channels, respectively, and let $k_h$ and $k_w$ be the height and width of the kernel. To get an output with multiple channels, we can create a kernel tensor of shape $c_i\\times k_h\\times k_w$ for *every* output channel. "
      ],
      "id": "56264cab"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "- concatenate them on the output channel dimension, so that the shape of the convolution kernel is $c_o\\times c_i\\times k_h\\times k_w$. In cross-correlation operations, the result on each output channel is calculated from the convolution kernel corresponding to that output channel and takes input from all channels in the input tensor.\n",
        "- implement a cross-correlation function to [**calculate the output of multiple channels**] as shown below.\n"
      ],
      "id": "e121be71"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def corr2d_multi_in_out(X, K):\n",
        "    ## Iterate through the 0th dimension of K, and each time, perform\n",
        "    ## cross-correlation operations with input X. All of the results are\n",
        "    ## stacked together\n",
        "    return d2l.stack([corr2d_multi_in(X, k) for k in K], 0)"
      ],
      "id": "ce4b4bfc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- We construct a trivial convolution kernel with 3 output channels by concatenating the kernel tensor for `K` with `K+1` and `K+2`.\n"
      ],
      "id": "74286278"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "K = d2l.stack((K, K + 1, K + 2), 0)\n",
        "K.shape"
      ],
      "id": "c5a51ec6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---"
      ],
      "id": "b96f2b7a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Below, we perform cross-correlation operations on the input tensor `X` with the kernel tensor `K`. Now the output contains 3 channels. The result of the first channel is consistent with the result of the previous input tensor `X` and the multi-input channel, single-output channel kernel.\n"
      ],
      "id": "a4560e40"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "corr2d_multi_in_out(X, K)"
      ],
      "id": "c74c4f48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## $1\\times 1$ Convolutional Layer\n",
        "- At first, a [**$1 \\times 1$ convolution**], i.e., $k_h = k_w = 1$, does not seem to make much sense. \n",
        "- After all, a convolution correlates adjacent pixels. A $1 \\times 1$ convolution obviously does not. Nonetheless, they are popular operations that are sometimes included in the designs of complex deep networks :cite:`Lin.Chen.Yan.2013,Szegedy.Ioffe.Vanhoucke.ea.2017` Let's see in some detail what it actually does.\n",
        "- Because the minimum window is used, the $1\\times 1$ convolution loses the ability of larger convolutional layers to recognize patterns consisting of interactions among adjacent elements in the height and width dimensions. The only computation of the $1\\times 1$ convolution occurs on the channel dimension.\n",
        "- :numref:`fig_conv_1x1` shows the cross-correlation computation using the $1\\times 1$ convolution kernel with 3 input channels and 2 output channels.\n",
        "- Note that the inputs and outputs have the same height and width. Each element in the output is derived from a linear combination of elements *at the same position* in the input image.\n",
        "- You could think of the $1\\times 1$ convolutional layer as constituting a fully connected layer applied at every single pixel location to transform the $c_i$ corresponding input values into $c_o$ output values. "
      ],
      "id": "04406f9d"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "- Because this is still a convolutional layer, the weights are tied across pixel location. Thus the $1\\times 1$ convolutional layer requires $c_o\\times c_i$ weights (plus the bias). Also note that convolutional layers are typically followed by nonlinearities. This ensures that $1 \\times 1$ convolutions cannot simply be  folded into other convolutions. \n",
        "  \n",
        "![The cross-correlation computation uses the $1\\times 1$ convolution kernel with 3 input channels and 2 output channels. The input and output have the same height and width.](../img/conv-1x1.svg)\n",
        " \n",
        "\n",
        "- Let's check whether this works in practice: we implement a $1 \\times 1$ convolution using a fully connected layer. \n",
        "- The only thing is that we need to make some adjustments to the data shape before and after the matrix multiplication.\n",
        "\n",
        "---"
      ],
      "id": "0177d737"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def corr2d_multi_in_out_1x1(X, K):\n",
        "    c_i, h, w = X.shape\n",
        "    c_o = K.shape[0]\n",
        "    X = d2l.reshape(X, (c_i, h * w))\n",
        "    K = d2l.reshape(K, (c_o, c_i))\n",
        "    ## Matrix multiplication in the fully connected layer\n",
        "    Y = d2l.matmul(K, X)\n",
        "    return d2l.reshape(Y, (c_o, h, w))"
      ],
      "id": "1fa8177a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- When performing $1\\times 1$ convolutions, the above function is equivalent to the previously implemented cross-correlation function `corr2d_multi_in_out`. Let's check this with some sample data.\n"
      ],
      "id": "961169ec"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = d2l.normal(0, 1, (3, 3, 3))\n",
        "K = d2l.normal(0, 1, (2, 3, 1, 1))\n",
        "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
        "Y2 = corr2d_multi_in_out(X, K)\n",
        "assert float(d2l.reduce_sum(d2l.abs(Y1 - Y2))) < 1e-6"
      ],
      "id": "2f528ba4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discussion\n",
        "\n",
        "- Channels allow us to combine the best of both worlds: MLPs that allow for significant nonlinearities and convolutions that allow for *localized* analysis of features. In particular, channels allow the CNN to reason with multiple features, such as edge and shape detectors at the same time. \n",
        "-  also offer a practical trade-off between the drastic parameter reduction arising from translation invariance and locality, and the need for expressive and diverse models in computer vision. \n",
        "- flexibility comes at a price. Given an image of size $(h \\times w)$, the cost for computing a $k \\times k$ convolution is $\\mathcal{O}(h \\cdot w \\cdot k^2)$. For $c_i$ and $c_o$ input and output channels respectively this increases to $\\mathcal{O}(h \\cdot w \\cdot k^2 \\cdot c_i \\cdot c_o)$. \n",
        "- For a $256 \\times 256$ pixel image with a $5 \\times 5$ kernel and $128$ input and output channels respectively\n",
        "-  amounts to over 53 billion operations (we count multiplications and additions separately).\n",
        "-  Later: effective strategies to cut down on the cost, e.g., by requiring the channel-wise operations to be block-diagonal, leading to architectures such as ResNeXt :cite:`Xie.Girshick.Dollar.ea.2017`. \n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Assume that we have two convolution kernels of size $k_1$ and $k_2$, respectively \n",
        "   (with no nonlinearity in-between).\n",
        "    1. Prove that the result of the operation can be expressed by a single convolution.\n",
        "    1. What is the dimensionality of the equivalent single convolution?\n",
        "    1. Is the converse true, i.e., can you always decompose a convolution into two smaller ones?\n",
        "1. Assume an input of shape $c_i\\times h\\times w$ and a convolution kernel of shape \n",
        "   $c_o\\times c_i\\times k_h\\times k_w$, padding of $(p_h, p_w)$, and stride of $(s_h, s_w)$.\n",
        "    1. What is the computational cost (multiplications and additions) for the forward propagation?\n",
        "    1. What is the memory footprint?\n",
        "    1. What is the memory footprint for the backward computation?\n",
        "    1. What is the computational cost for the backpropagation?\n",
        "1. By what factor does the number of calculations increase if we double the number of input channels \n",
        "   $c_i$ and the number of output channels $c_o$? What happens if we double the padding?\n"
      ],
      "id": "1eddd4d1"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "1. Are the variables `Y1` and `Y2` in the last example of this section exactly the same? Why?\n",
        "1. Express convolutions as a matrix multiplication, even when the convolution window is not $1 \\times 1$? \n",
        "1. Your task is to implement fast convolutions with a $k \\times k$ kernel. One of the algorithm candidates \n",
        "   is to scan horizontally across the source, reading a $k$-wide strip and computing the $1$-wide output strip \n",
        "   one value at a time. The alternative is to read a $k + \\Delta$ wide strip and compute a $\\Delta$-wide \n",
        "   output strip. Why is the latter preferable? Is there a limit to how large you should choose $\\Delta$?\n",
        "1. Assume that we have a $c \\times c$ matrix. \n",
        "    1. How much faster is it to multiply with a block-diagonal matrix if the matrix is broken up into $b$ blocks?\n",
        "    1. What is the downside of having $b$ blocks? How could you fix it, at least partly?\n",
        "\n",
        "## Pooling\n",
        "- in many cases our ultimate task asks some global question about the image, e.g., *does it contain a cat?* Consequently, the units of our final layer  should be sensitive to the entire input.\n",
        "- By gradually aggregating information, yielding coarser and coarser maps, we accomplish this goal of ultimately learning a global representation, while keeping all of the advantages of convolutional layers at the intermediate layers of processing. \n",
        "- The deeper we go in the network, the larger the receptive field (relative to the input) to which each hidden node is sensitive. Reducing spatial resolution  accelerates this process, since the convolution kernels cover a larger effective area. \n",
        "- Moreover, when detecting lower-level features, such as edges (as discussed in :numref:`sec_conv_layer`), we often want our representations to be somewhat invariant to translation. \n",
        "- For instance, if we take the image `X` with a sharp delineation between black and white and shift the whole image by one pixel to the right, i.e., `Z[i, j] = X[i, j + 1]`, then the output for the new image `Z` might be vastly different.\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "ab89ad54"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The edge will have shifted by one pixel. In reality, objects hardly ever occur exactly at the same place. In fact, even with a tripod and a stationary object, vibration of the camera due to the movement of the shutter might shift everything by a pixel or so (high-end cameras are loaded with special features to address this problem).\n",
        "- This section introduces *pooling layers*, which serve the dual purposes of mitigating the sensitivity of convolutional layers to location and of spatially downsampling representations.\n"
      ],
      "id": "d5337b88"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from d2l import torch as d2l\n",
        "import torch\n",
        "from torch import nn"
      ],
      "id": "a5f97495",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Maximum Pooling and Average Pooling\n",
        "- Like convolutional layers, *pooling* operators consist of a fixed-shape window that is slid over all regions in the input according to its stride, computing a single output for each location traversed by the fixed-shape window (sometimes known as the *pooling window*).\n",
        "- However, unlike the cross-correlation computation of the inputs and kernels in the convolutional layer, the pooling layer contains no parameters (there is no *kernel*). \n",
        "- Instead, pooling operators are deterministic, typically calculating either the maximum or the average value of the elements in the pooling window. \n",
        "- These operations are called *maximum pooling* (*max-pooling* for short) and *average pooling*, respectively.\n",
        "- *Average pooling* is essentially as old as CNNs. The idea is akin to  downsampling an image. \n",
        "- Rather than just taking the value of every second (or third)  pixel for the lower resolution image, we can average over adjacent pixels to obtain  an image with better signal to noise ratio since we are combining the information from multiple adjacent pixels. "
      ],
      "id": "3bcf9fe2"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "- *Max-pooling* was introduced in  :citet:`Riesenhuber.Poggio.1999` in the context of cognitive neuroscience to describe  how information aggregation might be aggregated hierarchically for the purpose  of object recognition, and an earlier version in speech recognition :cite:`Yamaguchi.Sakamoto.Akabane.ea.1990`. \n",
        "- In almost all cases, max-pooling, as it is also referred to, is preferable. \n",
        "- In both cases, as with the cross-correlation operator, we can think of the pooling window as starting from the upper-left of the input tensor and sliding across the input tensor from left to right and top to bottom. \n",
        "- At each location that the pooling window hits, it computes the maximum or average value of the input subtensor in the window, depending on whether max or average pooling is employed.\n",
        "\n",
        "![Max-pooling with a pooling window shape of $2\\times 2$. The shaded portions are the first output element as well as the input tensor elements used for the output computation: $\\max(0, 1, 3, 4)=4$.](../img/pooling.svg){width=300px}\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "e2de8f36"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- output tensor in :numref:`fig_pooling`  has a height of 2 and a width of 2. The four elements are derived from the maximum value in each pooling window:\n",
        "\n",
        "$$\n",
        "\\max(0, 1, 3, 4)=4,\\\\\n",
        "\\max(1, 2, 4, 5)=5,\\\\\n",
        "\\max(3, 4, 6, 7)=7,\\\\\n",
        "\\max(4, 5, 7, 8)=8.\\\\\n",
        "$$"
      ],
      "id": "2f832dc3"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "- More generally, we can define a $p \\times q$ pooling layer by aggregating over  a region of said size. Returning to the problem of edge detection,  we use the output of the convolutional layer as input for $2\\times 2$ max-pooling.\n",
        "- Denote by `X` the input of the convolutional layer input and `Y` the pooling layer output.  Regardless of whether or not the values of `X[i, j]`, `X[i, j + 1]`,  `X[i+1, j]` and `X[i+1, j + 1]` are different,  the pooling layer always outputs `Y[i, j] = 1`.\n",
        "- That is to say, using the $2\\times 2$ max-pooling layer, we can still detect if the pattern recognized by the convolutional layer moves no more than one element in height or width. \n",
        "- In the code below, we (**implement the forward propagation of the pooling layer**) in the `pool2d` function. This function is similar to the `corr2d` function in :numref:`sec_conv_layer`. \n",
        "\n",
        "\n",
        "---"
      ],
      "id": "4aeb903c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- However, no kernel is needed, computing the output as either the maximum or the average of each region in the input.\n"
      ],
      "id": "1542c302"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def pool2d(X, pool_size, mode='max'):\n",
        "    p_h, p_w = pool_size\n",
        "    Y = d2l.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
        "    for i in range(Y.shape[0]):\n",
        "        for j in range(Y.shape[1]):\n",
        "            if mode == 'max':\n",
        "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
        "            elif mode == 'avg':\n",
        "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
        "    return Y"
      ],
      "id": "00b4d350",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- can construct the input tensor `X` in :numref:`fig_pooling` to [**validate the output of the two-dimensional max-pooling layer**].\n"
      ],
      "id": "1960cb05"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = d2l.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
        "pool2d(X, (2, 2))"
      ],
      "id": "4ac018bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Also, we experiment with (**the average pooling layer**).\n"
      ],
      "id": "4dad6926"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pool2d(X, (2, 2), 'avg')"
      ],
      "id": "6f0c8900",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Padding and Stride\n",
        "\n",
        "- As with convolutional layers, pooling layers change the output shape. And as before, we can adjust the operation to achieve a desired output shape by padding the input and adjusting the stride.\n",
        "- We can demonstrate the use of padding and strides in pooling layers via the built-in two-dimensional max-pooling layer from the deep learning framework. \n",
        "- We first construct an input tensor `X` whose shape has four dimensions, where the number of examples (batch size) and number of channels are both 1.\n"
      ],
      "id": "935cf1a2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = d2l.reshape(d2l.arange(16, dtype=d2l.float32), (1, 1, 4, 4))\n",
        "X"
      ],
      "id": "d76c9c63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "- pooling aggregates information from an area, (**deep learning frameworks default to matching pooling window sizes and stride.**) For instance, if we use a pooling window of shape `(3, 3)` we get a stride shape of `(3, 3)` by default.\n"
      ],
      "id": "1536b97c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pool2d = nn.MaxPool2d(3)\n",
        "## Pooling has no model parameters, hence it needs no initialization\n",
        "pool2d(X)"
      ],
      "id": "09e6cc2b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- as expected, [**the stride and padding can be manually specified**] to override framework defaults if needed.\n"
      ],
      "id": "5a54434a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
        "pool2d(X)"
      ],
      "id": "91ba3f65",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---"
      ],
      "id": "efdd46b4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Of course, we can specify an arbitrary rectangular pooling window with arbitrary height and width respectively, as the example below shows.\n"
      ],
      "id": "8132328f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pool2d = nn.MaxPool2d((2, 3), stride=(2, 3), padding=(0, 1))\n",
        "pool2d(X)"
      ],
      "id": "92512606",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Multiple Channels\n",
        "\n",
        "- When processing multi-channel input data, [**the pooling layer pools each input channel separately**], rather than summing the inputs up over channels as in a convolutional layer. \n",
        "- This means that the number of output channels for the pooling layer is the same as the number of input channels. Below, we will concatenate tensors `X` and `X + 1` on the channel dimension to construct an input with 2 channels.\n"
      ],
      "id": "7001f8e7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = d2l.concat((X, X + 1), 1)\n",
        "X"
      ],
      "id": "b251048f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "As we can see, the number of output channels is still 2 after pooling.\n"
      ],
      "id": "46c307b0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
        "pool2d(X)"
      ],
      "id": "aab1e3ef",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- Pooling is an exceedingly simple operation. It does exactly what its name indicates, aggregate results over a window of values. \n",
        "- All convolution semantics, such as strides and padding apply in the same way as they did previously. \n",
        "- Note that pooling is indifferent to channels, i.e., it leaves the number of channels unchanged and it applies to each channel separately. Lastly, of the two popular pooling choices, max-pooling is preferable to average pooling, as it confers some degree of invariance to output. A popular choice is to pick a pooling window size of $2 \\times 2$ to quarter the spatial resolution of output. \n",
        "- Note that there are many more ways of reducing resolution beyond pooling. For instance, in stochastic pooling :cite:`Zeiler.Fergus.2013` and fractional max-pooling :cite:`Graham.2014` aggregation is combined with randomization. \n",
        "- This can slightly improve the accuracy in some cases. Lastly, as we will see later with the attention mechanism, there are more refined ways of aggregating over outputs, e.g., by using the alignment between a query and representation vectors. \n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Implement average pooling through a convolution. \n",
        "1. Prove that max-pooling cannot be implemented through a convolution alone. \n",
        "1. Max-pooling can be accomplished using ReLU operations, i.e., $\\mathrm{ReLU}(x) = \\max(0, x)$.\n",
        "    1. Express $\\max (a, b)$ by using only ReLU operations.\n",
        "    1. Use this to implement max-pooling by means of convolutions and ReLU layers. \n",
        "    1. How many channels and layers do you need for a $2 \\times 2$ convolution? How many for a $3 \\times 3$ convolution. \n",
        "\n",
        "\n",
        "---"
      ],
      "id": "ac9609bc"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. What is the computational cost of the pooling layer? Assume that the input to the pooling layer is of size $c\\times h\\times w$, the pooling window has a shape of $p_h\\times p_w$ with a padding of $(p_h, p_w)$ and a stride of $(s_h, s_w)$.\n",
        "3. Why do you expect max-pooling and average pooling to work differently?\n",
        "4. Do we need a separate minimum pooling layer? Can you replace it with another operation?\n",
        "5. We could use the softmax operation for pooling. Why might it not be so popular?\n",
        "\n",
        "## Convolutional Neural Networks (LeNet)\n",
        "\n",
        "- We now have all the ingredients required to assemble a fully-functional CNN. In our earlier encounter with image data, we applied a linear model with softmax regression (:numref:`sec_softmax_scratch`) and an MLP (:numref:`sec_mlp-implementation`) to pictures of clothing in the Fashion-MNIST dataset. To make such data amenable we first flattened each image from a $28\\times28$ matrix into a fixed-length $784$-dimensional vector, and thereafter processed them in fully connected layers. Now that we have a handle on convolutional layers, we can retain the spatial structure in our images. As an additional benefit of replacing fully connected layers with convolutional layers, we will enjoy more parsimonious models that require far fewer parameters. \n",
        "- In this section, we will introduce *LeNet*, among the first published CNNs to capture wide attention for its performance on computer vision tasks. The model was introduced by (and named for) Yann LeCun, then a researcher at AT&T Bell Labs, for the purpose of recognizing handwritten digits in images :cite:`LeCun.Bottou.Bengio.ea.1998`. This work represented the culmination of a decade of research developing the technology. "
      ],
      "id": "7fe571a8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "- In 1989, LeCun's team published the first study to successfully train CNNs via backpropagation :cite:`LeCun.Boser.Denker.ea.1989`.\n",
        "- At the time LeNet achieved outstanding results matching the performance of support vector machines, then a dominant approach in supervised learning, achieving an error rate of less than 1% per digit. \n",
        "- LeNet was eventually adapted to recognize digits for processing deposits in ATM machines. To this day, some ATMs still run the code that Yann LeCun and his colleague Leon Bottou wrote in the 1990s!\n"
      ],
      "id": "84c8678d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from d2l import torch as d2l\n",
        "import torch\n",
        "from torch import nn"
      ],
      "id": "fb28ce67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "## LeNet\n",
        "\n",
        "- LeNet (LeNet-5) consists of two parts: \n",
        "- (i) a convolutional encoder consisting of two convolutional layers; and \n",
        "- (ii) a dense block consisting of three fully connected layers\n",
        "- The architecture is summarized in :numref:`img_lenet`.\n",
        "\n",
        "![Data flow in LeNet. The input is a handwritten digit, the output a probability over 10 possible outcomes.](../img/lenet.svg){center=True width=600px}\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "be492027"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The basic units in each convolutional block are a convolutional layer, a sigmoid activation function, and a subsequent average pooling operation. Note that while ReLUs and max-pooling work better, these discoveries had not yet been made at the time. Each convolutional layer uses a $5\\times 5$ kernel and a sigmoid activation function. These layers map spatially arranged inputs to a number of two-dimensional feature maps, typically increasing the number of channels. The first convolutional layer has 6 output channels, while the second has 16. Each $2\\times2$ pooling operation (stride 2) reduces dimensionality by a factor of $4$ via spatial downsampling. The convolutional block emits an output with shape given by (batch size, number of channel, height, width). \n",
        "- In order to pass output from the convolutional block to the dense block, we must flatten each example in the minibatch. In other words, we take this four-dimensional input and transform it into the two-dimensional input expected by fully connected layers: "
      ],
      "id": "f3835910"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "- as a reminder, the two-dimensional representation that we desire uses the first dimension to index examples in the minibatch and the second to give the flat vector representation of each example.\n",
        "- LeNet's dense block has three fully connected layers, with 120, 84, and 10 outputs, respectively. \n",
        "- Because we are still performing classification, the 10-dimensional output layer corresponds to the number of possible output classes.\n",
        "- While getting to the point where you truly understand what is going on inside LeNet may have taken a bit of work, hopefully the following code snippet will convince you that implementing such models with modern deep learning frameworks is remarkably simple. We need only to instantiate a `Sequential` block and chain together the appropriate layers, using Xavier initialization as introduced in :numref:`subsec_xavier`.\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "15cccc08"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def init_cnn(module):  #@save\n",
        "    \"\"\"Initialize weights for CNNs.\"\"\"\n",
        "    if type(module) == nn.Linear or type(module) == nn.Conv2d:\n",
        "        nn.init.xavier_uniform_(module.weight)"
      ],
      "id": "007e0b43",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class LeNet(d2l.Classifier):  #@save\n",
        "    \"\"\"The LeNet-5 model.\"\"\"\n",
        "    def __init__(self, lr=0.1, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(120), nn.Sigmoid(),\n",
        "            nn.LazyLinear(84), nn.Sigmoid(),\n",
        "            nn.LazyLinear(num_classes))"
      ],
      "id": "71e72a63",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- take some liberty in the reproduction of LeNet insofar as we replace the Gaussian activation layer by a softmax layer. This greatly simplifies the implementation, not the least due to the fact that the Gaussian decoder is rarely used nowadays. Other than that, this network matches the original LeNet-5 architecture."
      ],
      "id": "83181c35"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "- Let's see what happens inside the network. By passing a single-channel (black and white) $28 \\times 28$ image through the network and printing the output shape at each layer, we can [**inspect the model**] to make sure that its operations line up with what we expect from :numref:`img_lenet_vert`.\n",
        "\n",
        "![Compressed notation for LeNet-5.](../img/lenet-vert.svg)\n",
        ":label:`img_lenet_vert`\n"
      ],
      "id": "53bd7a34"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "@d2l.add_to_class(d2l.Classifier)  #@save\n",
        "def layer_summary(self, X_shape):\n",
        "    X = d2l.randn(*X_shape)\n",
        "    for layer in self.net:\n",
        "        X = layer(X)\n",
        "        print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n",
        "        \n",
        "model = LeNet()\n",
        "model.layer_summary((1, 1, 28, 28))"
      ],
      "id": "4a70b79d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---"
      ],
      "id": "679c877f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- height and width of the representation at each layer throughout the convolutional block is reduced (compared with the previous layer). \n",
        "- The first convolutional layer uses 2 pixels of padding to compensate for the reduction in height and width that would otherwise result from using a $5 \\times 5$ kernel.\n",
        "- As an aside, the image size of $28 \\times 28$ pixels in the original MNIST OCR dataset is a result of *trimming* 2 pixel rows (and columns) from the original scans that measured $32 \\times 32$ pixels. \n",
        "- done primarily to save space (a 30% reduction) at a time when Megabytes mattered."
      ],
      "id": "d7c58ffe"
    },
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "\n",
        "- In contrast, the second convolutional layer forgoes padding, and thus the height and width are both reduced by 4 pixels. \n",
        "- As we go up the stack of layers, the number of channels increases layer-over-layer from 1 in the input to 6 after the first convolutional layer and 16 after the second convolutional layer.\n",
        "- However, each pooling layer halves the height and width. Finally, each fully connected layer reduces dimensionality, finally emitting an output whose dimension matches the number of classes.\n",
        "\n",
        "## Training\n",
        "\n",
        "- Now that we have implemented the model, let's [**run an experiment to see how the LeNet-5 model fares on Fashion-MNIST**]. \n",
        "- While CNNs have fewer parameters, they can still be more expensive to compute than similarly deep MLPs because each parameter participates in many more multiplications.\n",
        "- If you have access to a GPU, this might be a good time to put it into action to speed up training. \n",
        "- Note that the `d2l.Trainer` class takes care of all details. By default, it initializes the model parameters on the available devices.\n",
        "\n",
        "---"
      ],
      "id": "ccb249d7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Just as with MLPs, our loss function is cross-entropy, and we minimize it via minibatch stochastic gradient descent.\n"
      ],
      "id": "3972b9bd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
        "data = d2l.FashionMNIST(batch_size=128)\n",
        "model = LeNet(lr=0.1)\n",
        "model.apply_init([next(iter(data.get_dataloader(True)))[0]], init_cnn)\n",
        "trainer.fit(model, data)"
      ],
      "id": "3cdec2db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- moved from the MLPs of the 1980s to the CNNs of the 1990s and early 2000s. The architectures proposed, e.g., in the form of LeNet-5 remain meaningful, even to this day. It is worth comparing the error rates on Fashion-MNIST achievable with LeNet-5 both to the very best possible with MLPs (:numref:`sec_mlp-implementation`) and those with significantly more advanced architectures such as ResNet (:numref:`sec_resnet`). LeNet is much more similar to the latter than to the former. One of the primary differences, as we shall see, is that greater amounts of computation afforded significantly more complex architectures.\n",
        "- second difference is the relative ease with which we were able to implement LeNet. What used to be an engineering challenge worth months of C++ and assembly code, engineering to improve SN, an early Lisp based deep learning tool :cite:`Bottou.Le-Cun.1988`, and finally experimentation with models can now be accomplished in minutes. It is this incredible productivity boost that has democratized deep learning model development tremendously. In the next chapter we will follow down this rabbit to hole to see where it takes us.\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Let's modernize LeNet. Implement and test the following changes:\n",
        "    1. Replace the average pooling with max-pooling.\n",
        "    1. Replace the softmax layer with ReLU.\n",
        "2. Try to change the size of the LeNet style network to improve its accuracy in addition to max-pooling and ReLU.\n",
        "    1. Adjust the convolution window size.\n",
        "    1. Adjust the number of output channels.\n",
        "    1. Adjust the number of convolution layers.\n",
        "    1. Adjust the number of fully connected layers.\n",
        "    1. Adjust the learning rates and other training details (e.g., initialization and number of epochs.)"
      ],
      "id": "b7ebe7b1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "3. Try out the improved network on the original MNIST dataset.\n",
        "4. Display the activations of the first and second layer of LeNet for different inputs (e.g., sweaters and coats).\n",
        "5. What happens to the activations when you feed significantly different images into the network (e.g., cats, cars, or even random noise)?\n"
      ],
      "id": "37750f77"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}